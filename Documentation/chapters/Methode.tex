\chapter{Methode}
Die Methode, um die Fragestellung zu beantworten besteht aus zwei Teilen. Im
ersten Teil werden die Kriterien gefunden, nach denen die Leistung des Programms
evaluiert werden kann. Im zweiten Teil wird das Computerprogramm realisiert, das
bezogen auf die Kriterien die gewünschte Leistung erbringt. Die Antworten auf die
Unterfragen, zum Beispiel wie ein solches Programm aussehen kann, laufen in den
Prozess mit ein. 

Es gibt verschiedene Kriterien für die Leistung eines Computerprogramms, das
Menschen beim Zeichnen einer Zahl nachahmt. Einige Kriterien sind untereinander
kombinierbar, andere schliessen sich gegenseitig aus. So entstehen verschiedene
Versionen des Programmes, deren Leistung auf verschiedenen Kriterien beruht.


\section{Grundprogramm}
Es ist Teil der Methode, ein Computerprogramm zu entwickeln, das bezogen auf ein
Kriterium ein möglichst gutes Ergebnis erzielt. Reinforcement Learning
Algortihmen stellen diese Funktion bereit. Auch wenn sich das Kriterium ändert,
wonach sich das Computerprogramm richtet, kann der Reinforcement Learning
Algorithmus dahinter grösstenteils gleich bleiben. Der passende Reinforcement
Learning Algorithmus ist somit ein geeignetes Grundprogramm, worauf alle
Erweiterungen stützen.

Das Grundprogramm basiert auf dem Reinforcement Algorithmus aus dem Programm
Doodle-SDQ von Tao Zhou et Al (2018). Das Grundprogramm in dieser Arbeit
übernimmt die Architektur, bezogen auf die Form der Eingabe, der Ausgabe und den
versteckten Ebenen des Neuronalen Netz. 

\subsection*{Doodle-SDQ als Basis}
Bei der Umgebung hantelt es sich, wie bei Doodle-SDQ, um eine Zeichenfläche,
worauf sich der Agent frei bewegen kann. 

Die Zahlen, die nachgezeichnet werden sollen, stammen aus dem MNIST Datenset und
haben somit eine Grösse von 28x28 Pixeln. Die Fläche, worauf sich der Agent
bewegen kann, hat somit auch eine Grösse von 28x28 Pixeln. die "globale" Eingabe
in das Neuronale Netz ändert sich bis auf diese neue Grösse der Bilder nicht.

die Bilder, wie auch die Zeichenfläche, haben die Datenstruktur einer Bitmap. Es
handelt sich also um eine 28x28 Matrix, wobei jedes Element eine Null oder eine
Eins ist. Eine Null repräsentiert einen schwarzen Pixel an dieser Stelle im Bild
und eine Eins einen weissen Pixel.

Die Position des Agenten hat die Datenstruktur einer Liste mit zwei Elementen,
die die horizontalen und vertikalen Koordinaten auf der Zeichenfläche
repräsentieren

Die lokale Eingabe, also das nahe Umfeld um den Agenten schrumpft von 11x11
Pixeln auf 5x5 Pixeln. Somit schrumpft gleichzeitig der Actionspace des Agenten
von 2*11*11 = 242 Aktionen auf 2*5*5 50 Aktionen. Das bedeutet für den Agenten,
dass er sich pro Schritt um maximal zwei Pixel von seiner Position pro Schritt
bewegen kann. Diese bewegung kann der Agent wie in Doodle-SDQ entweder
zeichnend oder nicht zeichnend ausführen.

Falls der Agent die Aktion zeichnend ausführt, zieht das Programm einen Strich
zwischen der alten und der neuen Position. Mit anderen Worten werden alle Pixel
in der Zeichenfläche zwischen den beiden Positionen weiss gemacht. der Strich
hat eine festgelegte Breite von 3 Pixeln.

Am Anfang jeder Episode, also bei jeder neuen Zahl, die gezeichnet werden soll,
startet der Agent in einer zufälligen Position im nicht zeichnenden Zustand.

Aktionen des Agenten, die ihn über die vorgegebene Zeichenfläche hinaus
positionieren würden, sind nicht zulässig. Diese Aktionen können vom Agenten
nicht gewählt werden und ihr optimaler Q-Wert ist als 0 definiert. Das hat zur
Folge, dass nach der Trainingsphase die allermeisten unzulässigen Aktionen einen
tiefen Q-Wert haben. Das senkt die Wahrscheinlichkeit, dass der Agent versucht,
eine unzulässige Aktion zu wählen.

der Reward wird wie in Doodle-SDQ aus den neu übereinstimmenden Pixeln zwischen
der Vorlage und der Zeichenfläche pro Schritt definiert. Der Reward besteht
allerdings aus einer relativen, prozentualen Übereinstimmung der Pixel anstelle
eines absoluten Rewards pro Pixel. Auf die Eigenschaften und
die Unterschiede dieser beiden Reward-Funktionen wird im nächsten Abschnitt
eingegangen (siehe Evaluierung).
Anders als bei Doodle-SDQ bekommt der Agent keine negativen Rewards für langsame Bewegungen. 

% Wird Rare-Exploration verwendet?  Wenn ja: Todo
%- Umgebung (drawline Funktion, Bilder nur als Bitmaps, Translation der Aktion, Render, Anfang in random Position)
%- Agent (Eingabe 28x28, Ausgabe 7x7, rareexploration, Illegale Aktionen gegen 0 gehen lassen, (Relativer Reward = Kriterium zur Leistung der KI. prozentuale Übereinstimmung der Pixel. -> Genauer beschrieben im Kapitel Evaluierung der Leistung)
%- Unterschiede (Keine Stroke Demonstration, kein Training auf Geschwindigkeit)

\subsection*{Präparierung der Daten und Training}
Das in dieser Arbeit verwendete MNIST Datenset besteht aus 42000 Bildern von handgeschriebenen Zahlen. 



- Daten Präparierung (Datenset, auf Schwarz weiss , 1 0 begrenzen, abgrenzung Test und Train Phase)
    Total dataset: 42000
    Training: 36000
    Testing: 6000
- Trainingsdaten 
    3 Epochen
    4000 Episoden (heisst 4000 Bilder, die zufällig aus Datenset gewählt werden und vor jeder Epoche gemischt werden) -> 400 Bilder pro Nummer
    64 Steps (Stift kann sich 64 mal bewegen)

    Replay buffer speichert 200 Episoden (grösse 12800 Frames -> Steps)

    -> Total Training: 780'800 Frames
    Training in jedem 4. Schritt. mit Batchsize von 64



\section{Evaluierung der Leistung}
    Um Erweiterungen der KI mit der Grundversion vergleichen zu können, müssen Kriterien aufgestellt werden. 
    Algorithmus braucht reward. Anhaltspunkt was gut ist
    
    \subsection*{Messbare Leistung}
    Messbare Leistungen können als Reward dienen. Objektive angaben über Verhalten von Agent
    
    - Pixelähnlichkeit (-> Im Grundprogramm verwendet)
    - Geschwindigkeit (Wann Fertig oder Durchschnittsgeschwindigkeit)
    
    - Unterschied zwischen Reward, tatsächlicher Leistung
      Leistung kann durch gesammelter Reward pro Episode gemessen werden. in Reward können verschiedene Faktoren kommen (also nicht nur Pixelähnlichkeit)
      Fortschritt während Trainingsphase kann durch Akkumulierter Reward gemessen werden (Durchschnitt der letzten 12 Episoden Beispiel Grundprogramm)
      Durch Akkumulierter Reward können Unterschiede in Leistung erfasst werden. Weil sich aber Reward zwischen Versionen ändert kann nicht als Objektiver Vergleich verwendet werden
    
    \subsection*{nicht Messbare Leistungen}
    Ziel: Eine KI. Ein Programm, das Menschen nachahmt. 
    Kann Menschen besser oder schlechter nachahmen -> Leistung
    Schwer messbar. Subjektiv. Auf Beobachtungen gestützt

    (Probleme mit Grundprogramm?)
    
    Wichtigstes Prinzip: Schwungvoll. -> Hohe Geschwindigkeit, wenige bis keine Abbrüche
        - Implementierung von Physik. 
        - Erkennbarkeit über Genauigkeit


\section{Physikalische Umgebung}
% Art von Physik: Kinematik. Addierung von Geschwindigkeit 2 Dimensionale
% Kinematik. Stift bewegt sich nur auf Blatt -> einziges 3D Element ist hebung und
% senkung von Stift. Wie bis Anhin in 2 States geregelt. (Also kein Druck -> Entfernung von menschlichem Zeichnen)
Bei der physikalischen Umgebung für die Untersuchung handelt es sich um ein sehr
simples inertial System. Also ein System in dem die Kinematik gilt. Das Ziel
dieser Veränderung ist es die Anzahl möglicher Aktionen des Agenten zu
reduzieren, wodurch es einfacher werden soll diese zu erlernen. Zusätzlich soll
die simulierte Umgebung auch einer reellen Situation näher kommen und somit sich
mehr an das menschliche Zeichnen annähern.

Die Umgebung nimmt Kraftvektoren entgegen und berechnet darauf hin die neue
Stiftposition auf der Zeichenebene. Der Kraftvektor wird gemäss der Gesetze der
Kinematik zu der schon bestehenden Geschwindigkeit hinzugefügt
($\frac{\vec{F}}{m}\cdot t=\vec{v}$). Pro Zeitschritt wird die Geschwindigkeit
auch kleiner durch eine simulierte Reibung ($m\cdot g \cdot \mu = F_R$). Für die
Position wird immer die Geschwindigkeit zu der jetzigen Position addiert. Somit
wird als physikalische Zeit zwischen den Zeitschritten $t=1$ genommen.

Wie aus den Formeln herauszulesen ist, wurde den dem Stift und auch der Umgebung
physikalische Eigenschaften ($m$, $g$, $\mu$) gegeben, welche für ein optimales
Ergebnis angepasst wurden.

% TODO: Bleibt das noch so mit nur 2 States. Wird nicht vielleicht auch noch die z-Achse mehr eingebaut?

\subsection*{Training auf physikalische Umstände}
% Ausgabe in Form von Kraftvektoren in Kreis angeordnet. Kraftvektoren
% beeinflussen Geschwindigkeit des Agenten. Position in jedem Schritt wird über
% Geschwindigkeit bestimmt. Durchgehende Reibung wirkt auf Agenten
% -> Physik beruht ausnahmslos auf Dynamik und Kinematik. Vereinfachung durch Annahmen

% Geschwindigkeit Als Eingabe (-> Absolute Werte, oder verschiebung des Patch)
% Problem von zu hoher Geschwindigkeit (-> Lösung durch negative Belohnung)
Als Eingabe für die physikalische Umgebung, wurden Kraftvektoren verwendet, für
die sich der Agent in seiner Aktion entscheidet. Da die Aktion nur ein einfacher
Integer ist, hat jeder Kraftvektor einen eigenen Index bekommen. Es gibt
insgesamt $4$ Richtungen in die der Agent sich bewegen kann. Der Stift wird
somit immer in eine dieser Richtungen um die Einheit $1$ beschleunigt. Somit
gibt es eigentlich $4$ einzigartige Aktionen. Allerdings soll der Agent sich
auch entscheiden können gar nicht zu zeichnen oder stärker zu zeichnen, wodurch
nochmals $4\cdot(\underbrace{1}_{\text{nicht zeichnen}} +
\underbrace{s}_{\text{stärke des Zeichnens}})$ So sind die Möglichkeiten der
Aktionen des Agenten viel stärker begrenzt als in der ursprünglichen Variante.

Es stellte sich zu beginn des Training heraus, dass der Agent das Konzept der
Geschwindigkeit noch nicht verstanden hat. Um dieses Problem zu lösen wurde die
Geschwindigkeit als Kontext zur Observation hinzugefügt und somit als Eingabe
des Netzes verwendet. Dies brachte bessere Ergebnisse, allerdings gelang es noch
bessere Ergebnisse zu erziehlen, in dem man die Geschwindigkeit nicht roh als
Zahlen mitgibt, sondern den lokalen Patch in die Richtung der Geschwindigkeit,
also der nächsten Position des Agenten verschiebt.

Ein weiteres Problem während dem Training war, dass der Agent viel zu hohe
Geschwindigkeiten angenommen hat und somit oft aus der Zeichenebene verschwinden
wollte. Dieses Problem wurde durch die Reward Function gelöst, sodass er bei
solchem Verhalten bestraft wird.

% TODO: Bin mir nicht mehr ganz sicher, ob all diese Angaben stimmen
% TODO: Übersetzung Lokalen Patch nachschauen
% TODO: War negativer Reward wirklich die Lösung?


\section{Schrifterkennung als Kriterium}
Möglichkeit einer neuen messbaren Leistung
-> Annäherung an Menschliches Verhalten weil Erkennbarkeit > Genauigkeit


\subsection*{Training auf bessere Erkennbarkeit}
- Erkennung von MNIST Netz als Reward. 
verschiedene Varianten (MNIST Training only, Pixelreward mit MNIST am Schluss, Pixelreward - MNIST reward Verlauf, MNIST zu wenigen Schritten)


\section{Optimierung}
% Baysian Optimization. Hyperparameter Optimierung. Vlt. Geschwindigkeitsoptimierung des Algorithmus
