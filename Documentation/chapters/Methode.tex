\chapter{Methode}
Die Methode, um die Fragestellung zu beantworten besteht aus zwei Teilen. Im
ersten Teil werden die Kriterien gefunden, nach denen die Leistung des Programms
evaluiert werden kann. Im zweiten Teil wird das Computerprogramm realisiert, das
bezogen auf die Kriterien die gewünschte Leistung erbringt. Die Unterfragen, zum
Beispiel wie ein solches Programm aussehen kann, laufen zwangsläufig in den
Prozess mit ein. 

Es gibt verschiedene Kriterien für die Leistung eines Computerprogramms, das
Menschen beim Menschen beim Zeichnen einen Strichbildes oder eines Symbols
nachahmt. Einige Kriterien sind untereinander kombinierbar, andere schliessen
sich gegenseitig aus. So entstehen verschiedene Versionen des Programmes, deren
Leistung auf verschiedenen Kriterien beruht.


\section{Grundprogramm}
Es ist Teil der Methode, ein Computerprogramm zu entwickeln, das bezogen auf ein
Kriterium ein möglichst gutes Ergebnis erzielt. Reinforcement Learning
Algortihmen stellen diese Funktion bereit. Auch wenn sich das Kriterium ändert,
wonach sich das Computerprogramm richtet, kann der Reinforcement Learning
Algorithmus dahinter grösstenteils gleich bleiben. Der passende Reinforcement
Learning Algorithmus ist somit ein geeignetes Grundprogramm, worauf alle
Erweiterungen stützen.

Das Grundprogramm basiert auf dem Reinforcement Algorithmus aus dem Programm
Doodle-SDQ von Tao Zhou et Al (2018). 


\subsection*{Doodle-SDQ als Basis}
- Umgebung (drawline Funktion, Bilder nur als Bitmaps, Translation der Aktion, Render, Anfang in random Position)
- Agent (Eingabe 28x28, Ausgabe 7x7, rareexploration, Illegale Aktionen gegen 0 gehen lassen, (Relativer Reward = Kriterium zur Leistung der KI. prozentuale Übereinstimmung der Pixel. -> Genauer beschrieben im Kapitel Evaluierung der Leistung)
- Unterschiede (Keine Stroke Demonstration, kein Training auf Geschwindigkeit)

\subsection*{Präparierung der Daten und Training}
- Daten Präparierung (Datenset, auf Schwarz weiss , 1 0 begrenzen, abgrenzung Test und Train Phase)
    Total dataset: 42000
    Training: 36000
    Testing: 6000
- Trainingsdaten 
    3 Epochen
    4000 Episoden (heisst 4000 Bilder, die zufällig aus Datenset gewählt werden und vor jeder Epoche gemischt werden) -> 400 Bilder pro Nummer
    64 Steps (Stift kann sich 64 mal bewegen)

    Replay buffer speichert 200 Episoden (grösse 12800 Frames -> Steps)

    -> Total Training: 780'800 Frames
    Training in jedem 4. Schritt. mit Batchsize von 64



\section{Evaluierung der Leistung}
    Um Erweiterungen der KI mit der Grundversion vergleichen zu können, müssen Kriterien aufgestellt werden. 
    Algorithmus braucht reward. Anhaltspunkt was gut ist
    
    \subsection*{Messbare Leistung}
    Messbare Leistungen können als Reward dienen. Objektive angaben über Verhalten von Agent
    
    - Pixelähnlichkeit
    - Geschwindigkeit (Wann Fertig oder Durchschnittsgeschwindigkeit)
    
    - Unterschied zwischen Reward, tatsächlicher Leistung
      Leistung kann durch gesammelter Reward pro Episode gemessen werden. in Reward können verschiedene Faktoren kommen (also nicht nur Pixelähnlichkeit)
      Fortschritt während Trainingsphase kann durch Akkumulierter Reward gemessen werden (Durchschnitt der letzten 12 Episoden Beispiel Grundprogramm)
      Durch Akkumulierter Reward können Unterschiede in Leistung erfasst werden. Weil sich aber Reward zwischen Versionen ändert kann nicht als Objektiver Vergleich verwendet werden
    
    \subsection*{nicht Messbare Leistungen}
    Ziel: Eine KI. Ein Programm, das Menschen nachahmt. 
    Kann Menschen besser oder schlechter nachahmen -> Leistung
    Schwer messbar. Subjektiv. Auf Beobachtungen gestützt
    
    Wichtigstes Prinzip: Schwungvoll. -> Hohe Geschwindigkeit, wenige bis keine Abbrüche
        - Implementierung von Physik. 
        - Erkennbarkeit über Genauigkeit


\section{Physikalische Umgebung}
% Art von Physik: Kinematik. Addierung von Geschwindigkeit 2 Dimensionale
% Kinematik. Stift bewegt sich nur auf Blatt -> einziges 3D Element ist hebung und
% senkung von Stift. Wie bis Anhin in 2 States geregelt. (Also kein Druck -> Entfernung von menschlichem Zeichnen)
Bei der physikalischen Umgebung für die Untersuchung handelt es sich um ein sehr
simples inertial System. Also ein System in dem die Kinematik gilt. Das Ziel
dieser Veränderung ist es die Anzahl möglicher Aktionen des Agenten zu
reduzieren, wodurch es einfacher werden soll diese zu erlernen.

Die Umgebung nimmt Kraftvektoren entgegen und berechnet darauf hin die neue
Stiftposition auf der Zeichenebene. Der Kraftvektor wird gemäss der Gesetze der
Kinematik zu der schon bestehenden Geschwindigkeit hinzugefügt
($\frac{\vec{F}}{m}\cdot t=\vec{v}$). Pro Zeitschritt wird die Geschwindigkeit
auch kleiner durch eine simulierte Reibung ($m\cdot g \cdot \mu = F_R$). Für die
Position wird immer die Geschwindigkeit zu der jetzigen Position addiert. Somit
wird als physikalische Zeit zwischen den Zeitschritten $t=1$ genommen.

Wie aus den Formeln herauszulesen ist, wurde den dem Stift und auch der Umgebung
physikalische Eigenschaften ($m$, $g$, $\mu$) gegeben, welche für ein optimales
Ergebnis angepasst wurden.

% TODO: Bleibt das noch so mit nur 2 States. Wird nicht vielleicht auch noch die z-Achse mehr eingebaut?

\subsection*{Training auf physikalische Umstände}
% Ausgabe in Form von Kraftvektoren in Kreis angeordnet. Kraftvektoren
% beeinflussen Geschwindigkeit des Agenten. Position in jedem Schritt wird über
% Geschwindigkeit bestimmt. Durchgehende Reibung wirkt auf Agenten
% -> Physik beruht ausnahmslos auf Dynamik und Kinematik. Vereinfachung durch Annahmen

% Geschwindigkeit Als Eingabe (-> Absolute Werte, oder verschiebung des Patch)
% Problem von zu hoher Geschwindigkeit (-> Lösung durch negative Belohnung)
Als Eingabe für die physikalische Umgebung, wurden Kraftvektoren verwendet, für
die sich der Agent in seiner Aktion entscheidet. Da die Aktion nur ein einfacher
Integer ist, hat jeder Kraftvektor einen eigenen Index bekommen. Es gibt
insgesamt $4$ Richtungen in die der Agent sich bewegen kann. Der Stift wird
somit immer in eine dieser Richtungen um die Einheit $1$ beschleunigt. Somit
gibt es eigentlich $4$ einzigartige Aktionen. Allerdings soll der Agent sich
auch entscheiden können gar nicht zu zeichnen oder stärker zu zeichnen, wodurch
nochmals $4\cdot(\underbrace{1}_{\text{nicht zeichnen}} +
\underbrace{s}_{\text{stärke des Zeichnens}})$ So sind die Möglichkeiten der
Aktionen des Agenten viel stärker begrenzt als in der ursprünglichen Variante.

Es stellte sich zu beginn des Training heraus, dass der Agent das Konzept der
Geschwindigkeit noch nicht verstanden hat. Um dieses Problem zu lösen wurde die
Geschwindigkeit als Kontext zur Observation hinzugefügt und somit als Eingabe
des Netzes verwendet. Dies brachte bessere Ergebnisse, allerdings gelang es noch
bessere Ergebnisse zu erziehlen, in dem man die Geschwindigkeit nicht roh als
Zahlen mitgibt, sondern den lokalen Patch in die Richtung der Geschwindigkeit,
also der nächsten Position des Agenten verschiebt.

Ein weiteres Problem während dem Training war, dass der Agent viel zu hohe
Geschwindigkeiten angenommen hat und somit oft aus der Zeichenebene verschwinden
wollte. Dieses Problem wurde durch die Reward Function gelöst, sodass er bei
solchem Verhalten bestraft wird.

% TODO: Bin mir nicht mehr ganz sicher, ob all diese Angaben stimmen
% TODO: Übersetzung Lokalen Patch nachschauen
% TODO: War negativer Reward wirklich die Lösung?


\section{Schrifterkennung als Kriterium}
Möglichkeit einer neuen messbaren Leistung
-> Annäherung an Menschliches Verhalten weil Erkennbarkeit > Genauigkeit


\subsection*{Training auf bessere Erkennbarkeit}
- Erkennung von MNIST Netz als Reward. 
verschiedene Varianten (MNIST Training only, Pixelreward mit MNIST am Schluss, Pixelreward - MNIST reward Verlauf, MNIST zu wenigen Schritten)


\section{Optimierung}
% Baysian Optimization. Hyperparameter Optimierung. Vlt. Geschwindigkeitsoptimierung des Algorithmus
