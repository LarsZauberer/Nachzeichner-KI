\chapter{Methode}
Die Methode, um die Fragestellung zu beantworten, besteht aus zwei Teilen. Im
ersten Teil werden die Kriterien gefunden, nach denen die Leistung des Programms
evaluiert werden kann. Im zweiten Teil wird das Computerprogramm realisiert, das
bezogen auf die Kriterien die gewünschte Leistung erbringt. Die Antworten auf
die Unterfragen, zum Beispiel wie ein solches Programm aussehen kann, laufen in
den Prozess mit ein. 

Es gibt verschiedene Kriterien für die Leistung eines Computerprogramms, das
Menschen beim Zeichnen einer Zahl nachahmt. Einige Kriterien sind untereinander  % TODO: Zahl evt. ausbessern
kombinierbar, andere schliessen sich gegenseitig aus. So entstehen verschiedene
Versionen des Programmes, deren Leistung auf verschiedenen Kriterien beruht.


\section{Evaluierung der Leistung}
In diesem Abschnitt sind die Kriterien definiert, nach denen die Leistung des
Computerprogramms bestimmt wird. Der Aufbau des Computerprogrammes richtet sich
nach diesen Kriterien. Es gibt zwei Kategorien von Kriterien: messbare Kriterien
und nicht messbare Kriterien. Zwischen den Kategorien gibt es weitere Kriterien,
die teilweise messbar sind.

\subsection*{Messbare Kriterien}
Messbare Kriterien sind in diesem Fall objektiv. Das heisst sie sind nicht
abhängig von der Wahrnehmung eines Betrachters. Messbare Kriterien sind durch
berechenbare Zahlenwerte definiert und stützen auf Daten, die direkt aus dem
Computerprogramm hervorgehen. Aus diesen Gründen eignen sich messbare Kriterien
als Reward-Funktionen für einen Reinforcement Learning Algorithmus. Eine
Reward-Funktion berechnet allerdings nur den Unterschied im Wert des Kriteriums
zwischen zwei Schritten anstelle des eigentlichen Wertes (siehe Th RL)  % TODO: Ref setzen

Ein Beispiel für ein messbares Kriterium ist die \emph{Genauigkeit} der
Zeichnung. Spezifischer ist damit die Anzahl sich unterscheidender Pixel
zwischen der Vorlage und der Zeichnung gemeint. Es wird explizit nicht die
Anzahl der übereinstimmenden Pixel bestimmt, sondern der sich unterscheidenden.
Das bedeutet, dass ein tieferer Wert dieses Kriteriums einer besseren Leistung
entspricht. Der Vorteil davon liegt darin, dass die Pixel, die bereits zwischen
einer leeren Zeichenfläche und der Vorlage übereinstimmen, nicht mitgezählt
werden. Die Reward-Funktion des Reinforcement Learning Algorithmus in Doodle-SDQ
(siehe Doodle-SDQ) basiert auf diesem Kriterium.  % TODO: Ref setzen
\emph{Die prozentuale Genauigkeit} ist ein weiteres messbares Kriterium, das auf
dem letzten Kriterium basiert. Der Unterschied ist, dass der Wert des Kriterium
nicht durch eine Anzahl Pixel bestimmt wird, sondern durch den prozentualen
Anteil der richtigen Pixel in der Zeichnung zur Vorlage. Dieses Kriterium wird
im nächsten Kapitel (siehe Grundprogramm) genauer behandelt.  % TODO: Ref setzen


\subsection*{Nicht messbare Kriterien}
Nicht messbare Leistungen sind subjektiv. Sie sind abhängig von der Wahrnehmung
eines Betrachters. Sie können nicht, oder nicht ausreichend genau durch einen
Zahlenwert definiert werden. Diese Kriterien funktionieren also nicht als eine
Reward-Funktion. Dadurch kann auch nicht objektiv beurteilt werden, wie gut die
Leistung des Computerprogrammes bezogen auf diese Kriterien ist. Die
\emph{Menschlichkeit} des Computerprogrammes ist ein Beispiel für ein nicht
messbares Kriterium. Trotzdem ist das Kriterium relevant für die Beantwortung
der Fragestellung dieser Untersuchung. Das Computerprogramm soll Menschen nachahmen.
Weil es nicht möglich ist, die Menschlichkeit des Computerprogrammes durch
Zahlen zu repräsentieren und den Algorithmus darauf zu trainieren, sind nur
qualitative Annäherungen möglich. 
\emph{Zeichnen in einer physikalischen Umgebung} ist eine mögliche Annäherung.
Die Führung eines Stiftes ist mit physikalischen Prozessen aus der Dynamik und
der Kinematik verbunden. Wie sehr diese physikalischen Prozesse die
Menschlichkeit des Computerprogrammes beeinflussen ist wie erwähnt nicht
messbar. Dieses Kriterium kann nur durch Beobachtungen und dem Vergleich mit
Erfahrungen bestimmt werden.

\subsection*{Teilweise messbare Kriterien}
Wie im letzten Abschnitt erwähnt sind bei dem Computerprogramm nur qualitative
Annäherungen an die Menschlichkeit möglich. Einige mögliche Annäherungen sind
jedoch teilweise messbar. Mit teilweise messbar ist hier gemeint, dass die
Kriterien durch einen Zahlenwert repräsentiert werden können, aber zuerst
subjektiv definiert werden müssen. Mit dem Zahlenwert in der Reward-Funktion
kann der Algorithmus auf diese Kriterien trainiert werden.

Ein Beispiel ist die \emph{Erkennbarkeit} der Zeichnung. Entweder ist die
richtige Zahl in der Zeichnung erkennbar oder nicht. Diese möglichen Ausgänge
können durch Zahlen, zum Beispiel Eins und Null, repräsentiert werden. Dafür
muss jedoch definiert sein, wann die Zahl erkennbar ist. 
Ein weiteres Beispiel ist die \emph{Geschwindigkeit} des Zeichnens, oder
spezifischer die Anzahl Schritte bis zur Fertigstellung der Zeichnung. Diese
Zahl ist messbar. Für dieses Kriterium muss jedoch definiert sein, wann die
Zeichnung fertig ist.

Es scheint so, dass für diese Kriterien eine menschliche Beurteilung nötig ist.
Als Ausblick: Dieser Umstand ist in beiden Fällen durch eine zweite
künstliche Intelligenz, die die menschliche Beurteilung nachahmt, umgehbar.

\subsection*{Wahl der Kriterien}
Folgende Kriterien werden im Verlauf der Methode weiterverfolgt:

\begin{itemize}
  \item Prozentuale Genauigkeit
  \item Zeichnen in einer physikalischen Umgebung
  \item Erkennbarkeit 
  \item Geschwindigkeit
\end{itemize}

Die nächsten Kapitel behandeln jeweils eines dieser Kriterien in dieser Reihenfolge.

% TODO: Ich finde die Section zwar wichtig, ist aber denke ich eigentlich zu kurz um eine eigene Section sein zu können.

\section{Grundprogramm}
Es ist Teil der Methode, ein Computerprogramm zu entwickeln, das bezogen auf ein
Kriterium, ein möglichst gutes Ergebnis erzielt. Reinforcement Learning  % TODO: Komma nach Kriterium nicht sicher
Algortihmen stellen diese Funktion bereit (siehe RL). Auch wenn sich das  % TODO: Ref setzen
Kriterium ändert, wonach sich das Computerprogramm richtet, kann der
Reinforcement Learning Algorithmus dahinter grösstenteils gleich bleiben. Der
passende Reinforcement Learning Algorithmus ist somit ein geeignetes  % TODO: Was ist genau mit passend gemeint?
Grundprogramm, worauf alle Erweiterungen stützen. 

\subsection*{Doodle-SDQ als Basis}
Das Grundprogramm basiert auf dem Reinforcement Algorithmus aus dem Programm
Doodle-SDQ von Tao Zhou et Al (2018) \cite{zhou_learning_2018}. Das
% TODO: Soll überhaupt so genau lange der Name des Projektes ausgeführt werden, wenn wir es eh Zitieren müssen. (Abschnitte sind zusammengefügt)
Grundprogramm in dieser Arbeit übernimmt die Architektur, bezogen auf die Form
der Eingabe, der Ausgabe und den versteckten Ebenen des Neuronalen Netz.

Bei der Umgebung handelt es sich, wie bei Doodle-SDQ, um eine Zeichenfläche,
worauf sich der Agent frei bewegen kann.
% TODO: Wieso ein neuer Abschnitt? Nicht etwas zu kurz

Die Zahlen, die nachgezeichnet werden sollen, stammen aus dem MNIST Datenset und
haben somit eine Grösse von $28\times28$ Pixeln. Die Fläche, worauf sich der Agent
bewegen kann, hat somit auch eine Grösse von $28\times28$ Pixeln. Die `globale' Eingabe
in das Neuronale Netz ändert sich bis auf diese neue Grösse der Bilder nicht.
% TODO: Wieso meinst du noch ändern?
% TODO: Sollte man da noch schreiben, dass keine Graustufen in den Bildern genommen werden.

Die Bilder, wie auch die Zeichenfläche, haben die Datenstruktur einer Bitmap. Es
handelt sich also um eine $28\times28$ Matrix, wobei jedes Element eine Null oder eine
Eins ist. Eine Null repräsentiert einen schwarzen (nicht gezeichneten) Pixel an dieser Stelle im Bild
und eine Eins einen weissen (gezeichneten) Pixel.

Die Position des Agenten hat die Datenstruktur einer Liste mit zwei Elementen,
die die horizontalen und vertikalen Koordinaten auf der Zeichenfläche
repräsentieren
% TODO: Wurde dafür nicht die Distmap, also eine Heatmap genutzt?

Die lokale Eingabe, also das nahe Umfeld um den Agenten schrumpft von
$11\times11$ Pixeln auf $7\times7$ Pixeln. Somit schrumpft gleichzeitig der
Actionspace des Agenten von $2\cdot11\cdot11 = 242$ Aktionen auf $2\cdot7\cdot7
= 98$ Aktionen. Das bedeutet für den Agenten, dass er sich pro Schritt um
maximal drei Pixel von seiner Position pro Schritt bewegen kann. Diese Bewegung
kann der Agent wie in Doodle-SDQ entweder zeichnend oder nicht zeichnend
ausführen.

Falls der Agent die Aktion zeichnend ausführt, zieht das Programm einen Strich
zwischen der alten und der neuen Position. Mit anderen Worten werden alle Pixel
in der Zeichenfläche zwischen den beiden Positionen weiss gemacht. Der Strich
hat eine festgelegte Breite von $3$ Pixeln.

Am Anfang jeder Episode, also bei jeder neuen Zahl, die gezeichnet werden soll,
startet der Agent in einer zufälligen Position im nicht zeichnenden Zustand.

Aktionen des Agenten, die ihn über die vorgegebene Zeichenfläche hinaus
positionieren würden, sind nicht zulässig. Diese Aktionen können vom Agenten
nicht gewählt werden und ihr optimaler Q-Wert ist als $0$ definiert. Das hat zur
Folge, dass nach der Trainingsphase die allermeisten unzulässigen Aktionen einen
tiefen Q-Wert haben. Das senkt die Wahrscheinlichkeit, dass der Agent versucht,
eine unzulässige Aktion zu wählen.
% TODO: Wie wurde das definiert im Code?

Die Reward-Funktion verwendet das Kriterium der Prozentualen Genauigkeit. (siehe eval messbare)  % TODO: Welchen Teil soll referenziert werden, der von der methode oder aus der diskussion
Spezifischer ist damit die prozentuale Übereinstimmung der weissen Pixel
zwischen der Vorlage und der Zeichnung gemeint. Wie bei Doodle-SDQ werden die
sich unterscheidenden Pixel gezählt. Zuerst wird die Anzahl der Pixel, die sich
zwischen der Vorlage und einer leeren Zeichenfläche unterscheiden, berechnet.
Diese Zahl $S_{max}$ entspricht allen weissen Pixeln in der Vorlage, da die
leere Zeichenfläche vollständig schwarz ist. Der Wert $K(t)$ des Kriteriums zu
einem bestimmten Schritt berechnet sich aus folgender Formel: 

$$ K(t) = 1 - \frac{S(t)}{S_{max}} $$  % TODO: Soll das eine saubere Equation werden? Dann kann sie auch referenziert werden

Dabei ist $S(t)$ die Anzahl sich underscheidender Pixel zu dem Schritt an dem
das Kriterium ausgewertet werden soll. Bei diesem Wert handelt es sich also um
die prozentuale (als Dezimalzahl) Übereinstimmung der Weissen Pixel. Dieser Wert
kann auch negativ sein, wenn weisse Pixel an Stellen gezeichnet werden, an denen
in der Vorlage keine sind. Diese Reward-Funktion hat Gegenüber von derjenigen in
Doodle-SDQ einen Vorteil. Die Leistung wird bei Vorlagen, die nur wenige weisse
Pixel haben (wo zum Beispiel die Zahl klein geschrieben ist), nicht kleiner
eingeschätzt, als bei Vorlagen mit vielen weissen Pixeln. So werden gewisse
Vorlagen anderen nicht vorgezogen. Anders als bei Doodle-SDQ bekommt der Agent
keine negativen Rewards für langsame Bewegungen. 
% TODO: Soll der Unterschied lieber in der Diskussion beschrieben werden?

Alle Aspekte des Grundprogrammes sind leicht veränderbar, um es für
Erweiterungen flexibel zu halten.

% TODO: Review von untenstehenden Fragen
% Wird Rare-Exploration verwendet?  Wenn ja: Todo
%- Umgebung (drawline Funktion, Bilder nur als Bitmaps, Translation der Aktion, Render, Anfang in random Position)
%- Agent (Eingabe 28x28, Ausgabe 7x7, rareexploration, Illegale Aktionen gegen 0 gehen lassen, (Relativer Reward = Kriterium zur Leistung der KI. prozentuale Übereinstimmung der Pixel. -> Genauer beschrieben im Kapitel Evaluierung der Leistung)
%- Unterschiede (Keine Stroke Demonstration, kein Training auf Geschwindigkeit)

\subsection*{Präparierung der Daten und Training}
Das in dieser Arbeit verwendete MNIST Datenset besteht aus $42000$ Bildern von
handgeschriebenen Zahlen zwischen Null und Neun. Die Bilder im Datenset sind als
Bitmap dargestellt, wobei jedes Element (jeder Pixel) einen Wert zwischen $0$ und
255 annimmt. Die Zahl räpresentiert einen Punkt auf dem Spektrum von Grautönen,
wobei 0 Schwarz ist und 255 Weiss. Diese Graustufen werden entfernt. Jeder Pixel
mit einem Wert über 0 übernimmt den Wert 1. So stimmen die Bilder mit den
Zeichnungen, die der Agent produzieren kann, überein.
% TODO Vielleicht oben bei der Änderungen der Daten eine Referenz setzen
% TODO: Muss vielleicht geändert werden, wenn Graustufen wieder hinzugefügt werden

Die Trainingsdaten bestehen aus $36000$ der $42000$ Bilder im Datenset. Die
restlichen $6000$ Bilder sind für die Testphase aufgehoben. Das Grundprogramm
trainiert mit $4000$ Bildern, von denen jede Zahl von Null bis Neun $400$ Bilder
ausmacht. Die restlichen Bilder in den Trainingsdaten sind für mögliche
Erweiterungen aufgehoben. Der Agent zeichnet jedes der $4000$ Bilder insgesamt
drei mal. Mit anderen Worten Läuft die Trainingsphase für $3$ Epochen mit jeweils
$4000$ Episoden. Der Agent macht $64$ Schritte pro Episode. Er kann sich also pro
Zeichnung $64$ mal bewegen. Insgesamt trainiert der Algorithmus somit auf der
Basis von $3\cdot4000\cdot64$ = $780'000$ Schritten. Der Replay Buffer speichert die
Schritte von $700$ Episoden. Somit speichert er $700\cdot64 = 44'800$ Schritte. Das
Training (des Neuronalen Netz) findet in jedem vierten Schritt statt mit einem  % TODO: Ist vierten Schritt oder Episode?
Batch von $64$ zufällig gewählten Schritten aus dem replay buffer.
% TODO: Sollten diese ganzen Zahlen und daten vielleicht als Tabelle dargestellt werden?
% TODO: Brauchen diese unteren Daten aus der Architektur mit den Hyperparametern ein eigenes Kapitel
% TODO: Manche Parameter müssen später nochmals geändert werden










% Ziel: Eine KI. Ein Programm, das Menschen nachahmt. 
% Kann Menschen besser oder schlechter nachahmen -> Leistung
% Schwer messbar. Subjektiv. Auf Beobachtungen gestützt

%     (Probleme mit Grundprogramm?)
    
%     Wichtigstes Prinzip: Schwungvoll. -> Hohe Geschwindigkeit, wenige bis keine Abbrüche
%         - Implementierung von Physik. 
%         - Erkennbarkeit über Genauigkeit


\section{Physikalische Umgebung}
% Art von Physik: Kinematik. Addierung von Geschwindigkeit 2 Dimensionale
% Kinematik. Stift bewegt sich nur auf Blatt -> einziges 3D Element ist hebung und
% senkung von Stift. Wie bis Anhin in 2 States geregelt. (Also kein Druck -> Entfernung von menschlichem Zeichnen)
Bei der physikalischen Umgebung für die Untersuchung handelt es sich um ein sehr
simples inertial System. Also ein System in dem die Gesetze der Kinematik
gelten. Das Ziel dieser Veränderung ist es die Anzahl möglicher Aktionen des
Agenten zu reduzieren, wodurch es einfacher werden soll diese zu erlernen.
Zusätzlich soll die simulierte Umgebung auch einer reellen Situation näher
kommen und somit sich mehr an das menschliche Zeichnen annähern.

Die Umgebung nimmt Kraftvektoren entgegen und berechnet darauf hin die neue
Stiftposition auf der Zeichenebene. Der Kraftvektor wird gemäss der Gesetze der
Kinematik zu der schon bestehenden Geschwindigkeit hinzugefügt. Jeder
Zeitschritt des Agenten repräsentiert in der Physik die vergangene Zeit $t=1$.
($\frac{\vec{F}}{m}\cdot t=\vec{v}$). Pro Zeitschritt wird die Geschwindigkeit
auch kleiner durch eine simulierte Reibung ($m\cdot g \cdot \mu = F_R$). Für die
Position wird immer die Geschwindigkeit zu der jetzigen Position addiert.

Wie aus den Formeln herauszulesen ist, besitzt der Stift und auch die Umgebung
physikalische Eigenschaften ($m$, $g$, $\mu$), welche für ein optimales Ergebnis
später optimiert werden.

Der Agent hat in der Umgebung die Möglichkeit haben seinen Stift mit
Kraftvektoren zu beschleunigen. Nicht nur eine Beschleunigung ist möglich,
sondern das Gleiten mit der restlichen Geschwindigkeit ist für den Agenten
möglich.

Um das menschlichte Zeichnen noch näher zu bringen kann sich der Agent
zusätzlich entscheiden, wie stark der Stift auf die Zeichenfläche drückt. Das
bewirkt, dass um den Stift herum Pixel zusätzlich bemalt werden. Zu der Stärker
kommt auch dazu, dass der Agent sich entscheiden kann, gar nicht auf das Papier
zu drücken, sondern sich ganz vom Papier zu heben und nicht zu zeichnen.

In der Realität müsste der Druck auch mit einer Beschleunigung gelöst werden,
was zur Vereinfachung weggelassen wird.

\subsection*{Training auf physikalische Umstände}
% Ausgabe in Form von Kraftvektoren in Kreis angeordnet. Kraftvektoren
% beeinflussen Geschwindigkeit des Agenten. Position in jedem Schritt wird über
% Geschwindigkeit bestimmt. Durchgehende Reibung wirkt auf Agenten
% -> Physik beruht ausnahmslos auf Dynamik und Kinematik. Vereinfachung durch Annahmen

% Geschwindigkeit Als Eingabe (-> Absolute Werte, oder verschiebung des Patch)
% Problem von zu hoher Geschwindigkeit (-> Lösung durch negative Belohnung)


Da die Aktiondes Agenten nur ein einfacher Integer ist, hat jeder Kraftvektor
einen eigenen Index bekommen. Es gibt insgesamt $20 + 1 (\text{gar nicht     %Der Action-space besteht aus 21 Aktionen im zeichnenden Zustand und die selben 21 Aktionen im nicht zeichnenden Zustand.  
bewegen})$ Richtungen in die der Agent sich bewegen kann. Der Stift wird somit %Die 21 Aktionen bestehen aus Kraftvektoren die in verschiedene Richtunge um den Agent zeigen.
immer in eine dieser Richtungen, um den Betrag $1$ beschleunigt. Mit den
Optionen des Drucks kommt die Formel für die Anzahl Aktionsmöglichkeiten heraus:
$21\cdot(\underbrace{1}_{\text{nicht zeichnen}} + \underbrace{s}_{\text{maximale
Stärke des Zeichnens}})$. So sind die Möglichkeiten der Aktionen des Agenten
viel stärker begrenzt als in der ursprünglichen Variante. % Von den 50 Aktionen der Basisversion auf 42 gesenkt

Es stellte sich zu beginn des Training heraus, dass der Agent das Konzept der
Geschwindigkeit noch nicht verstanden hat. Um dieses Problem zu lösen wurde die % Die Geschwindigkeit des Agenten zu einem gegebenen Zeitpunkt ist relevant für die Entscheidung der nächsten Aktion.
Geschwindigkeit als Kontext zur Observation hinzugefügt und somit als Eingabe   % Deswegen müssen Informationen über die Geschwindigkeit 
des Netzes verwendet. Dies brachte bessere Ergebnisse, allerdings gelang es noch
bessere Ergebnisse zu erziehlen, in dem man die Geschwindigkeit nicht roh als
Zahlen mitgibt, sondern den lokalen Patch in die Richtung der Geschwindigkeit,
also der nächsten Position des Agenten verschiebt.

Ein weiteres Problem während dem Training war, dass der Agent viel zu hohe
Geschwindigkeiten angenommen hat und somit oft aus der Zeichenebene verschwinden
wollte. Dieses Problem wurde durch die Reward Function gelöst, sodass er bei
solchem Verhalten bestraft wird.

% TODO: Übersetzung Lokalen Patch nachschauen
% TODO: War negativer Reward wirklich die Lösung?

\subsection*{Parameteroptimizerung}
Nach einigem Trainieren zeigt sich, dass der Agent am meisten Erfolg mit einer
maximalen Zeichenstärke von $1$ hat. Zur Optimierung der physikalischen
Parameter wird gleich, wie bei der Hyperparameteroptimierung (siehe % TODO: Ist das wird hier so gut gelöst?
\nameref{chap:Hyperparameter Optimierung}) der Baysian Algorithmus verwendet um
das bestmöglichste Ergebnis zu erzielen.

% TODO: Ist das Überhaupt eine gute Idee, weil wir so die Aufgabenstellung des menschlichen Zeichnens verfremden?




\section{Schrifterkennung als Kriterium}
Dieses Kapitel behandelt das Kriterium der Erkennbarkeit der gezeichneten Zahl.
Dieses Kriterium nähert qualitativ die Menschlichkeit der Zeichnung, im
Vergleich zum Kriterium der prozentualen Genauigkeit, weiter an. Die Annäherung
an menschliches Verhalten liegt dabei in der Annahme, dass für Menschen
Erkennbarkeit wichtiger als Genauigkeit ist. Diese Annahme erklärt zum Beispiel
unterschiede in der Handschrift von verschiedenen Menschen. Handgeschriebene
Zahlen sehen je nach Person verschieden aus, bleiben aber in den allermeisten
Fällen für andere Personen erkennbar. So wäre es auch für das Computerprogramm
profitabel, Erkennbarkeit der Genauigkeit vorzuziehen.

Das Computerprogramm benötigt eine Funktion, um die Erkennbarkeit einer Zahl zu
beurteilen. Diese Funktion ist durch Schrifterkennungssoftware, also eine
weitere künstliche Intelligenz, zugänglich (siehe Theorie Schrifterkennung). Die
in dieser Arbeit verwendete Schrifterkennungssoftware stammt von Vittorio Mazzia
und Francesco Salvetti (Link) Die Schrifterkennungssoftware erreicht bei einem
Test von 5000 Bildern aus den Trainingsdaten eine Genauigkeit von ......%

Eine Zahl gilt nur als erkannt, wenn das zugehörige Neuron einen Wert von über 0.9
hat. Das entspricht einer hohen Wahrscheinlichkeit zur korrekten Beurteilung. 

\subsection{Training auf Erkennbarkeit}
Mit der Schrifterkennungssoftware ist die Erkennbarkeit der Zeichnungen für jede
Version des Computerprogramms bestimmbar. Das Computerprogramm, beziehungsweise
der Reinforcement Learning Algorithmus, kann aber auch spezifisch auf dieses
Kriterium trainiert werden. 

Um das Computerprogramm auf Erkennbarkeit zu trainieren, beinhaltet die
Reward-Funktion eine Implementierung der Schrifterkennungssoftware. Die
Schrifterkennungssoftware beurteilt in jedem Schritt die erkannte Zahl in der
Vorlage und der aktuellen Zeichnung. Wenn die erkannte Zahl in der Vorlage und
der Zeichnung gleich ist, erhält der Agent einen Reward von $0.1$. In diesem
Zustand funktioniert die Reward-Funktion nicht. Das heisst, der Agent kann den
akkumulierten Reward nicht vergrössern. Für dieses Problem gibt es mindestens
zwei Gründe.

Der erste Grund liegt in der Einschätzung der Schrifterkennungssoftware, die
teilweise für einen menschlichen Betrachter Fragwürdig ist. Zum Beispiel schätzt
die Schrifterkennungssoftware eine leere Zeichenfläche mit einer hohen
Wahrscheinlichkeit als eine Eins ein. Das ist in diesem Fall ein Problem, weil
dadurch der Agent einen positiven Reward (eine Belohnung) für eine leere
Zeichenfläche erhält. Das stört das weitere Lernverhalten, indem es
wahrscheinlicher wird, dass der Agent nicht mehr zeichnet. Die verwendete Lösung
ist, die Schrifterkennungssoftware erst ab einer gewissen prozentualen
Genauigkeit einzusetzen. So löst die Schrifterkennungssoftware erst Rewards aus,
wenn die prozentuale Genauigkeit der Zeichnung mehr als 20\% beträgt. 

Der zweite Grund liegt in einer zu hohen Abhängigkeit auf den Zufall. Damit die
Schrifterkennungssoftware einen Reward auslösen kann, muss der Agent eine gut
erkennbare Zahl zeichnen. Diese Zeichnung muss durch Zufall entstehen, weil der
Agent zuvor ohne einen Reward nicht besser wird. Dieses Problem wird ebenfalls
durch eine Implementierung des Kriteriums der prozentualen Genauigkeit gelöst.
Dieses Kriterium ermöglicht in der Reward-Funktion, wie im Grundprogramm (siehe
Grundprogramm), einen positiven Reward selbst für kleine Verbesserungen. Dieser
Reward ist vor allem am Anfang des Trainings wichtig. Deswegen nimmt der Reward
über das ganze Training hinweg linear ab, während der Reward für eine richtig
erkannte Zahl linear zunimmt.


\section{Hyperparameteroptimierung}
\label{chap:Hyperparameter Optimierung}
% Baysian Optimization. Hyperparameter Optimierung. Vlt. Geschwindigkeitsoptimierung des Algorithmus
Zur Optimierung der Hyperparameter wird der Baysian Algorithmus verwendet. Es
wird ein bereits implementierung in Python genutzt, um eine schnellere
Ergebnisse zu erzielen. \cite{fernando_bayesian_2022}

Als Blackboxfunktion wurde das Agent im Environment genommen. Dieser trainiert
$4000$ Episoden und der Durchschnitt des Rewards der letzten 12 Episoden wird
als Wert zur optimierung verwendet. Um eine bessere Vergleichbarkeit dieser
Werte sicherzustellen, werden immer dieselben $4000$ Referenzbilder gezeigt.
