\chapter{Theoretische Grundlagen}
\label{chap:t}
Die theoretischen Grundlagen führen die Konzepte ein, die über die ganze Arbeit
hinweg Anwendung finden. Es handelt sich dabei um Zusammenfassungen. Die Theorie
wird auf den Teil reduziert, der für ein grundsätzliches Verständnis der Arbeit
nötig ist. Weitere Informationen werden sind in den referenzierten Quellen
einsehbar. Auch die verwendeten Fachbegriffe werden in diesem Kapitel
eingeführt. 

\section{Machine Learning}
\label{chap:t_ml}
Machine Learning ist ein Teilbereich der künstlichen Intelligenz. ``Künstliche
Intelligenz (KI) bezieht sich im Allgemeinen auf jedes menschenähnliche
Verhalten durch eine Maschine oder ein System'' [What is Artificial Intelligence] Mit Maschinen und                
Systemen sind in den allermeisten Fällen Computer, beziehungsweise die
steurenden Computerprogramme gemeint. Diese Computerprogramme bilden ein Modell
von menschlichem Verhalten. Machine Learning Modelle entwickeln (oder erlernen)
eine Mustererkennung durch die Analyse von Daten. [What is Machine Learning]
Mustererkennung bedeutet hier, dass der Algorithmus Zusammenhänge zwischen den
analysierten Daten erkennt und auf dieser Basis Vorhersagen treffen kann.
Vereinfacht gesagt, versucht ein Machine Learning Modell menschliches
Urteilsvermögen zu erlernen. [is human judgement necessary]

Ein Beispielproblem für ein Machine Learning Modell ist die Erkennung von
handschriftlichen Zahlen. Ein Computerprogramm soll durch den Input eines Bildes
mit einer handschriftlichen Zahl eine korrekte Beurteilung treffen, um welche
Zahl es sich handelt. Mit anderen Worten soll der Output des Computerprogrammes
der Zahl entsprechen, die auf dem Bild der Eingabe zu sehen ist (siehe autoref{recognition}). Jedes
Computerprogramm, das dieses Problem löst, fällt in den Bereich der künstlichen
Intelligenz. Machine Learning Modelle geben einen Ansatz für die Umsetzung eines
solchen Computerprogrammes.

%todo Bild recognition machine

Machine Learning Modelle, die das Beispielproblem lösen, basieren üblicherweie
auf Supervised Learning. Das ist ein Teilbereich von Machine Learning, wobei das
Machine Learning Modell aus Rückmeldungen der korrekten Beurteilung, der
Zielvariable, als Reaktion auf ihre eigenen Beurteilungen lernt. [datasolut] Die
Zielvariable muss dabei im Voraus für jeden Datenpunkt in den analysierten Daten
durch einen Menschen festgelegt sein. [ML 3.1] Ausgedrückt durch den Fachbegriff müssen die
Daten labeled sein. [2.1 labeled] Weitere Teilbereiche von Machine Learning sind
Unsupervised Learning und Reinforcement Learning. [supervised vs. unsupervised
vs. reinforce]. Beachte \nameref{chap:reinf} für eine ausgeprägtere Einführung
in Reinforcement Learning.

\subsection{Funktionsweise eines Machine Learning Modelles}
\label{sub:t_ml_func}
Dieser Abschnitt erklärt die Funktionsweise eines Machine Learning Modelles,
basierend auf dem Beispielproblem aus dem letzten Abschnitt (siehe\nameref{chap:t_ml}). 

Bei den Daten, die das Machine Learning Modell analysiert handelt es sich in
diesem Fall um das MNIST Datenset. [MNIST]  Dieses Datenset wurde vom NIST
(National Institute of Standards and Technology) in Amerika veröffentlicht und
beinhaltet $70'000$ Bilder von hangeschriebenen Zahlen. [NIST EMNIST] Jedes Bild
hat eine auflösung von $28\times28$ Pixeln. (siehe autoref{mnist})

%todo mnist bild
                
Ein Machine Learning Modell durchläuft ein Training gefolgt von einer Testphase.
[Train test split] Während dem Training erlernt das Modell die Mustererkennung,
um verlässliche Aussagen zu den Daten der Eingabe zu treffen. Die Testphase
misst die Genauigkeit des Modelles, also die Wahrscheinlichkeit, mit der das
Modell die richige Lösung zur Eingabe liefert. Nur in den seltensten Fällen
erreicht diese Genauigkeit $100\%$. Das Modell garantiert somit nicht die
richtige Lösung. Das Machine Learning Modell erlert die Mustererkennung während
dem Training durch die Analyse von Daten aus dem Datenset. Das Modell gibt zu
jedem Datenpunkt die Beurteilung, um welche Zahl es sich handelt. Das Datenset
ist labeled (siehe \nameref{chap:t_ml}). Falls die Beurteilung des Modelles
nicht mit der bekannten, korrekten Lösung übereinstimmt, passt sich das Modell
automatisch auf eine bestimmte Weise an. Dadurch soll die Beurteilungen für
zukünftige Datenpunkte genauer werden.

Machine Learning Modelle unfassen verschiedene Hyperparameter. Diese beschreiben
unter anderem wie lange das Training läuft oder wie stark sich das Modell nach
einer falschen Beurteilung anpasst. Diese Hyperparamter beeinflussen das
Lernverhalten des Modelles [Hyperparameter] aber ihr optimaler Wert ist im
Voraus nicht bekannt. Hyperparamter können durch Algorithmen, wie den Baysian
Algorithmus optimiert werden.

Zusammengefasst kann Ein Machine Learning Modell Daten Beurteilen und sich
selbst Anpassen, um die Beurteilungen zu verbessern. Künstliche Neuronale Netze
umfassen diese Funktionalität, und finden daher in Machine Learning Modellen
Anwendung.

\subsection{künstliche neuronale Netze}
\label{sub:t_ml_nn}
Ein neuronales Netz ist, im biologischen Sinne, "eine beliebige Anzahl Neuronen,
die miteinander Verbunden sind". [Neuro Wikipedia] Ein Beispiel für ein
neuronales Netz ist das menschliche Gehirn. Künstliche Neuronale Netze
modellieren biologische neuronale Netze in der Form von Programmcode [artificial
neural network]. Diese Arbeit behandelt künstliche neuronale Netze, nicht aber
biologische. Somit handelt es sich bei jedem erwähnten neuronalen Netz, um ein
künstliches neuronales Netz.

Der Grundbaustein eines neuronalen Netzes ist das Neuron. Im Modell stellt das
Neuron ein Objekt dar, das eine beliebiege Anzahl Inputs, aber nur einen Output
hat (siehe autoref{neuron}). [Concept artificial neuron] Input und Output sind
hierbei rationale Zahlen. Die Ausgabe des Neurons ist im einfachsten Modell, dem
Perceptron, grundsätzlich entweder 0 oder 1. Die Ausgabe ist 1, wenn die Summe
der Eingaben einen vorgegebenen Wert, den \emph{Bias} des Neurons überschreiten.
Ansonsten ist die Ausgabe gleich 0. Jede Eingabe hat ein \emph{Gewicht}, das
einer rationalen Zahl entspricht. Vor der Addition der Inputs wird jeder Input
mit seinem Gewicht multipliziert.  Die Grösse des Gewichts bestimmt somit den
Einfluss der zugehörigen Eingabe auf die Ausgabe des Neurons. [neural networks
deep learning][What is Perceptron]

%todo Bild A neuron

Neuronale Netze in Machine Learning Modellen verwenden kompliziertere Neuronen
als das Perceptron, wie zum Beispiel das Sigmoid-Neuron. Die Neuronen
unterscheiden sich in ihrer Activation Function und somit im Verhalten ihres
Outputs [activation function]. So nimmt der Output im Sigmoid-Neuron
beispielsweise auch Werte zwischen $0$ und $1$ an in einem stetigen Übergang
zwischen den beiden Grenzen (siehe autoref{sig vs perc}). [sigmoid Neuron]

%todo Bild sigmoid - perceptron funktion

Neuronale Netze sind Verbindungen dieser Neuronen. Dabei dient die Ausgabe eines
Neurons als eine Eingabe in ein anderes Neuron. Die Neuronen sind in
\emph{Layers} geordnet (siehe autoref{layers}). Neuronale Netze haben mindestens
eine \emph{Input Layer} und eine \emph{Output Layer}. [neural network deep
learning][Neural network backprop] Die Input Layer umfasst die Daten, zu dem das
neuronale Netz eine Beurteilung liefert sollte. Im Beispielproblem (siehe
\nameref{chap:t_ml}) bestände die Eingabe-Ebene aus $28\times28$ Neuronen, wobei
jedes Neuron die Graustufe (durch einen Wert von $0$ bis $255$) eines Pixels im
Bild beschreibt. Der Input ist in diesem Fall zweidimensional. Die Dimensionen
sind allerdings flexibel. Die Output Layer besteht im Beispiel aus $10$
Neuronen, wobei jedes Neuron einer Beurteilung entspricht (das fünfte Neuron
beschreibt zum Beispiel die Ziffer Fünf als Beurteilung). Dasjenige Neuron mit
dem höchsten Output entspricht der Beurteilung des neuronalen Netzes.

%todo Bild Layers

Zwischen der Input Layer und der Output Layers kann es weitere \emph{Hidden
Layers} geben. [What are hidden layers] Es gibt verschiedene Arten von Hidden
Layers, die verschidene Funktionen haben. Zwei der meist verwendeten Layers sind
Fully Connected (Dense) Layers und Convolutional Layers. [conv vs. fully
connected] In Fully Connected Layers dient jedes Neuron als Input für jedes
Neuron in der nächsten Layer. In Convolutional Layers trifft das nicht zu.
(siehe autoref{conv vs. full})Die Funktion von Convolutional Layers ist es,
wichtige Merkmale aus dem Input hervorzuheben. [convolutional neural network].
Concatenation Layers [concat] sind eine weitere Form von hidden Layers, die zwei
verschiedene Layers als Input haben und diese somit verbinden. Machine Learning
Modelle werden ab mehr als einer Hidden Layer als Deep Learning Modelle
bezeichnet. [deep learning]

%todo Bild conv vs full
%file:///C:/Users/robin/Zotero/storage/PAZJPWRZ/convolutional-layers-vs-fully-connected-layers-364f05ab460b.html

Ein Machine Learning Modell passt während der Trainingsphase (siehe
\nameref{sub:t_ml_func}) einzelne Gewichte im neuronalen Netz in der
Hoffnung an, dass die Genauigkeit der Beurteilung mit den angepassten Gewichten
grösser ist. Die Genaue Anpassung erfolgt in den meisten Machine Learning
Modellen durch den Backpropagation Algorithmus. [Backpropagation][backpropagation]

%todo ---------------------------------------------------------------------------------------------------------------
\section{Reinforcement Learning}
\label{chap:t_rl}
Reinforcement Learning bedeutet Lernen durch Interaktion mit der Umgebung.
[complete guide]. Genauer gesagt soll ein Machine Learning Modell durch
Rückmeldungen aus einer Umgebung ein bestimmtes Verhalten erlernen.

Reinforcement Learning Modelle führen somit die Umgebung ein. Anders als bei
Supervised Learning und Unsupervised Learning sind die Daten, aus denen das
Modell lernen soll, im Voraus nicht bekannt. Das liegt in der Natur in der
Umgebung, die häufig zu viele verschiedene Zustände einnehmen kann, als dass
diese in einem Datenset gesammelt werden könnten. Ein Machine Learning Modell
kann trotzdem aus einer Umgebung lernen, indem es selbst mit dieser interagiert
und dadurch Erfahrungen sammelt. [synopsis]

Die echte Welt kann ebenfalls als eine Umgebung angesehen werden. Der Mensch
wäre in diesem Fall der Machine learning Algorithmus. Der Mensch lernt durch
Interaktion mit seiner Umgebung die Eigenschaften dieser kennen. So lernt ein
Mensch die Schwerkraft durch das Hinfallen kennen. Durch diese Erfahrungen kann
der Mensch ein gewissen Verhalten, zum Beispiel das Laufen, erlernen.
Reinforcement Learning Modelle stellen dieses Lernverhalten nach. So verwendet
die Robotik häufig diesen Ansatz um einen Roboter laufen zu lassen. Die Umgebung
wodurch das reinforcement Learning Modell learnt ist dabei häufig nicht nicht
echte Welt, sondern eine simulierte Umgebung.


\subsection{Aufbau und Funktionsweise}
Dieser Abschnitt umfasst eine genauere Erklärung eines Reinforcement Learning
Modelles (in diesem Fall Deep Q-Learning) unter der Verwendung der korrekten
Fachbegriffe.

% Todo: Umgebung -> Environment
Ein Reinforcement Learning Agent umfasst eine \emph{Umgebung} und einen
\emph{Agent}. Der Agent ist dasjenige Element in der Umgebung, welches mit
dieser interagiert und daraus lernt. [S.B. s.53] Die Umgebung verändert sich in
Zeitschritten, genannt \emph{Steps}. In jedem Step führt der Agent eine
\emph{Action} aus, die die Umgebung beeinflusst. Die Entscheidung, welche Action
der Agent ausführt basiert auf einer \emph{Observation} [DQN s.2] der Umgebung. Die
Observation umfasst alle Daten der Umgebung, die für die Entscheidung des Agents
relevant sind. Die Entscheidung, welche Aktion der Agent ausführt, basiert auf
einem neuronalen Netz. Der Input in dieses neuronale Netz ist die Observation
der Umgebung und der Output beschreibt die action, die der Agent ausführt. Jedes
Neuron des Outputs beschreibt eine spezifische Action des Agents. Der Agent kann
somit nur eine feste Anzahl Actions ausführen. Diese Actions zusammen werden
\emph{Action-Space} [S.B s.67] genannt. Jede Action im Action-Space besitzt einen
\emph{Q-Value}, der dem Output des zugehörigen neurons entspricht [Q-Learning]. Die
Entscheidung, welche Action ausgeführt wird basiert auf der
\emph{Epsilon-Greedy} Strategie [S.B s.34]. Diese Strategie sieht vor, dass die
Entscheidung mit einer Wahrscheinlichkeit von $\epsilon$ auf eine zufällige
Action fällt. Ansonsten Fällt die Entscheidung auf diejenige Action mit dem
höchsten Q-Value. Die zufälligen Aktionen des Agents stellen eine Erkundung der
Umgebung dar.

Die Umgebung und somit auch der Agent werden durch die Actions des Agenten
beeinflusst. Dieser Einfluss wird durch die \emph{Reward-Function} gemessen. Die
Reward-Function gibt eine rationale Zahl, den \emph{Reward} aus [S.B s.75]. Umso grösser
der Reward, desto positiver ist der Effekt auf die Umgebung und umgekehrt. Ein
positiver Einfluss auf die Umgebung durch eine Action ist so definiert, dass der
Agent durch die Action das gewünschte Verhalten vorzeigt. Die Reward-Function
definiert, welches Verhalten welchen Reward erzielt. Der Q-Value der gewählten
Action wird mit dem Reward (und dem Maximalen Q-Value aus den nächsten möglichen
Actions) addiert. Diese Formel nennt sich Bellman-Gleichung [DQN s.3]. Der neue Q-Value          
nimmt somit einen kleineren Wert an wenn der Reward negativ ist und einen
grösseren Wert wenn der Reward positiv ist. Die Gewichte des neuronalen Netzes
werden daraufhin so angepasst, dass der Output für das Neuron, dessen Action
ausgeführt wurde, näher am neu berechneten Q-Value ist. Der schlussendliche
Effekt ist, dass Actions, die in einem gewissen Zustand der Umgebung einen
positiven Reward auslösen wahrscheinlicher werden und umgekehrt Actions mit
negativen Rewards unwahrscheinlicher werden. Der Agent versucht insgesamt durch
seine Actions einen möglichst hohen akkumulierten reward zu erzielen [S.B s.57]. Der
akkumulierte Reward entsprich der Summe der Rewards aus jedem Step

Das Trainingsphase läuft in \emph{Episodes} [S.B s.14]. Eine Episode umfasst
eine gewisse Anzahl Steps und am Anfang jeder Episode wird die Umgebung in einen
Startzustand zurückgesetzt. Die Resultate eines Steps, also Die Zustände der
Umgebung, die Actions und der Reward, werden in dem \emph{Replay-Buffer}
gespeichert. Dieser Enthält Speicherplatz für eine bestimmte Anzahl Steps.
Während dem Training werden zufällige Steps aus dem Replay-Buffer gewählt, auf
die das neuronale Netz trainiert. Das neurnale Netz trainiert also auf Daten aus
der Vergangenheit der Umgebung und des Agents. Diese Strategie nennt sich
experience replay. [DQN s.5][DQL] Ausserdem trainiert das neuronale Netz jeweils mit
einem \emph{Batch} an Steps, also mit einer gewissen Anzahl an Steps
gleichzeitig. Der Replay-Buffer und der Batch sichern zu, dass das neuronale
Netz mit einer grossen Varianz an Steps trainiert, was das Lernverhalten
stabiler macht [training with exp replay, batch].


\section{Verwandte Arbeiten und Themen}
\label{chap:t_verwandt}
Das Nachzeichnen von Strichbildern hängt allgemein mit dem Zeichnen von Bildern
durch einen Computer zusammen Es gibt verschiedene Ansätze, um einen Computer
zeichnen zu lassen. Ein häufiger Ansatz ist \emph{Stroke-Based Rendering}
[stroke based rendering]. Stroke-Based Rendering ist das Zeichnen von Bilder
durch das Platzieren von Elementen wie Strichen. Beispiele für Arbeiten in
diesem Bereich sind Strokenet [Strokenet] und ``Learning to Paint With
Model-based Deep Reinforcement Learning'' [learning to paint] Andere Ansätze
simulieren die Führung eines Stiftes. Ein Beispiel dafür ist das Programm
Doodle-SDQ [Doodle-SDQ]. Doodle-SDQ beschäftigt sich auch spezifischer mit dem
Nachzeichnen von Strichbildern und wird deswegen im nächsten Abschnitt weiter
behandelt.

\subsection{Doodle-SDQ}
Doodle-SDQ ist ein Computerprogramm, das durch ein Reinforcement Learning
Modell, spezifischer Deep Q-Learning, erlernt, Strichbilder aus dem Google
QuickDraw Datenset [Quickdraw Image rec] nachzuzeichnen. Nachfolgend sind die Aspekte von Doodle-SDQ
beschrieben, die für diese Arbeit relevant sind.

Die QuickDraw Bilder, die das Programm nachzeichnen soll, sind zu einer
einheitliche Grösse von $84\times84$ Pixeln verarbeiteitet. [Dood s.7] Der Agent kann sich
auf einer leeren Zeichenfläche von der selben Grösse bewegen und zeichnen. Die
Umgebung umfasst diese Zeichenfläche, den Agent und das abzuzeichnende Bild.

Der Agent kann sich durch eine Action in jedem Step auf einen beliebigen Pixel
in einem $11\times11$ Feld, in dessen Zentrum er ist, bewegen. Der Agent kann
ausserdem jede dieser Bewegungen im zeichnenden Zustand oder im nicht
zeichnenden Zustand machen. Der Action-Space hat somit insgesamt eine Grösse von
$2\cdot11\cdot11 = 242$ Actions. [Dood s.5] Im zeichnenden Zustand wird ein Strich auf der
Zeichenfläche zwischen der alten und der neuen Position des Agenten gezeichnet.
Der Agent begeht 100 Steps pro Episode. Eine neue Episdoe entspricht dabei einem
neuen Bild, das abgezeichnet werden soll

Der Input und der Output des neuronalen Netzes hat folgende Form: Der Input ist
in zwei Teile gegliedert: den Global Stream und den Local Stream. 
Der Global Stream hat eine Form von $28\times28\times4$. Der Input ist somit
dreidimensional. Die Form kann als 4 aufeindandergestapelte Bilder angesehen
werden, die jeweils eine Grösse von $28\times28$ Pixeln haben. Dabei beschreibt
eine relle Zahl den Wert von jedem Pixel in einem Bild. Das erste Bild im global
Stream ist die Vorlage, die abgezeichnet werden soll. Das zweite Bild ist die
Zeichenfläche im aktuellen Zustand. Das dritte Bild beschreibt die Position des
Agents durch seine relative Entfernung zu jedem Punkt auf der Zeichenfläche. Das
vierte Bild beschreibt, ob der Agent im zeichnenden Zustand ist oder nicht. Wenn
alle Pixel dieses letzten Bildes den Wert 1 haben, ist der Agent im zeichnenden
Zustand. Wenn Umgekehrt alle Pixel den Wert 0 haben, ist der Agent nicht im
zeichnenden Zustand. 
Der Local Stream hat eine Form von $11\times11\times2$ und ist somit auch
dreidimensional und beschreibt zwei gestapelte Bilder. Das erste Bild umfasst
die Vorlage in dem $11\times11$ Bereich, in dem sich der Agent in einem Schritt
bewegen kann. Das zweite Bild beschreibt den selben Bereich von der Zeichenfläche. [Dood s.4 f.]

Die Reward-Function bezieht sich auf die Differenz der Anzahl der
übereinstimmenden Pixel zwischen der Vorlage und Zeichenfläche zwischen zwei
Steps. [Dood s.6] Viele neu übereinstimmende Pixel entsprechen so einem hohen Reward für
einen Step. Wenn nach einem Step weniger Pixel übereinstimmen zuvor, entspricht
das einem negativen Reward. 

\section{Git und GitHub}
\label{chap:git_github}
Git und Github sind weit verbreitete Hilfsmittel für Software Entwickler. Git
ist ein Programm, während GitHub ein Service ist, der dieses Programm in der
Cloud zugänglich macht. GitHub hat zusätzliche Funktionen, die die Zusammenarbeit
zwischen mehreren Entwicklern erleichtern. Die genaue Funktion und das
Zusammenspiel dieser beiden Hilfsmittel wird nachfolgend erläutert.

\subsection{Git}
Git erkennt Veränderungen im Code eines Projektes und speichert diese
Veränderungen in einer neuen Version ab. Die einzelnen Versionen des Projektes
bleiben dabei zu jedem Zeitpunkt abrufbar. Dieses Konzept nennt sich Version
Control. [Version Control] Das Programm wurde 2005 von Linus Torvald entwickelt. 
Versionen des Projektes werden manuell durch einen Commit gespeichert. Es wird
empfohlen, nur jeweils ein bestimmtes Problem oder eine bestimmte Funktion pro
Commit anzugehen. Für grössere Funktionen oder Probleme kann ein Branch erstellt
werden. Ein Branch ermöglicht eine abgekapselte Entwicklung eines Projektes. [Git Branch] Zum
Beispiel kann das Projekt in mehreren Branches gleichzeitig und unabhängig von
einander entwickelt werden. Das übliche Vorgehen mit Branches sieht so aus, dass
aus einem Main Branch für jede grössere Funktion ein Branch erstellt wird.
Die Entwicklung der Funktion findet nur in dem zugehörigen Branch statt. Erst
wenn die Funktion fertig ist, wird der zugehörige Branch wieder mit dem Main
Branch zusammengeführt. Wenn die Arbeit in einem Branch scheitert, kann dieser
gelöscht werden, ohne den Main Branch zu beeinflussen. [Git How Branch]

\subsection{GitHub}
GitHub wurde 2008 von Chris Wanstrath, PJ Hyett, Scott Chacon und Tom
Preston-Werner entwickelt \cite{noauthor_github_2021}. 2018 wurde das
Unternehmen von Microsoft gekauft. GitHub ist ein Service, der Projekte, die mit
Git verwalten werden, in der Cloud speichert. Dadurch kann ein Projekt überall
und von beliebig vielen Personen entwickelt werden. GitHub betreibt eine
Webseite, über welche der Service verwendet werden kann.
\cite{noauthor_github_2021} 

GitHub besitzt verschiedene Hilfsmittel, die die Zusammenarbeit von Entwicklern
weiter vereinfachen. Beispiele dafür sind Issues und Project Boards. Diese
Hilfsmittel ermöglichen Organisation, Strukturierung und Arbeitsteilung. Ein
weiteres Hilfsmittel sind Pull Requests. Eine Pull Reqeust wird dann gestellt,
wenn die Arbeit an einem Branch fertig ist. Durch Pull Requests können die
Entwickler des Projektes die Funktionalität eines Branches überprüfen. Wenn ein
Branch nicht die gewünschte Aufgabe erfüllt, kann die Pull Request abgelehnt
werden. Erst wenn eine Pull Request angonommen wird, kann der Branch wieder in
den Main Branch zurückgeführt werden. [Pull Request]

