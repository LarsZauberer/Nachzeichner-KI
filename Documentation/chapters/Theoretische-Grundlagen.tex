\chapter{Theoretische Grundlagen}
\section{Machine Learning}
\label{chap:t_ml}
Machine Learning ist ein Teilbereich der künstlichen Intelligenz. "Der Begriff
`künstliche Intelligenz' beschreibt menschenähnliches Verhalten von Maschinen
oder Systemen". (ZITAT) Mit Maschinen und Systemen sind in den allermeisten Fällen       %Todo: Zitat quelle
Computer, beziehungsweise die steurenden Computerprogramme gemeint. Diese
Computerprogramme bilden ein Modell von menschlichem Verhalten. Machine Learning
Modelle entwickeln (oder erlernen) eine Mustererkennung durch die Analyse von
Daten. Mustererkennung bedeutet hier, dass der Algorithmus Zusammenhänge
zwischen den analysierten Daten erkennt und auf dieser Basis vorhersagen treffen
kann. Vereinfacht gesagt erlernt ein Machine Learning Modell menschliches Urteilsvermögen

Ein Beispielproblem für ein Machine Learning Modell ist die Erkennung von
handschriftlichen Zahlen. Ein Computerprogramm soll durch den Input eines
Bildes mit einer handschriftlichen Zahl eine korrekte Beurteilung treffen, um welche
Zahl es sich handelt. Mit anderen Worten soll der Output des Computerprogrammes
der Zahl entsprechen, die auf dem Bild der Eingabe zu sehen ist. Jedes
Computerprogramm, das dieses Problem löst, fällt in den Bereich der künstlichen
Intelligenz. Machine Learning Modelle geben einen Ansatz für die Umsetzung eines
solchen Computerprogrammes.

Machine Learning Modelle, die das Beispielproblem lösen basieren üblicherweie
auf Supervised Learning. Das ist ein Teilbereich von Machine Learning, wobei das
Machine Learning Modell aus Rückmeldungen der korrekten Beurteilung als Reaktion
auf ihre eigenen Beurteilungen lernt. Weitere Teilbereiche von Machine Learning
sind Unsupervised Learning und Reinforcement Learning. Reinforcement Learning
wird in dieser Arbeit noch weiter geschrieben.

\subsection*{Funktionsweise eines Machine Learning Modelles}
\label{sub:funktionsweise}
Dieser Abschnitt erklärt die Funktionsweise eines Machine Learning Modelles,
basierend auf dem Beispielproblem aus dem letzten Abschnitt (siehe
\nameref{chap:t_ml}). 

Bei den Daten, die das Machine Learning Modell analysiert handelt es sich in
diesem Fall um das MNIST Datenset. Dieses wurde vom NIST (National Institute of
Standards and Technology) in Amerika kreiert und beinhaltet $70'000$ Bilder von
hangeschriebenen Zahlen. Jedes Bild hat eine auflösung von $28\times28$ Pixeln.
(MNIST datenset)                                 %Todo: ref

Ein Machine Learning Modell durchläuft eine Trainingsphase gefolgt von einer
Testphase. In der Trainingsphase erlernt das Modell die Mustererkennung, um
verlässliche Aussagen zu den Daten der Eingabe zu treffen. Die Testphase misst
die Genauigkeit des Modelles, also die Wahrscheinlichkeit, mit der das Modell
die richige Lösung zur Eingabe liefert. Nur in den seltensten Fällen erreicht
diese Genauigkeit $100\%$. Das Modell garantiert somit nicht die richtige
Lösung.Das Machine Learning Modell erlert die Mustererkennung während der
Trainingsphase durch die Analyse von Daten aus dem Datenset. Das Modell gibt zu
jedem Datenpunkt die Beurteilung, um welche Zahl es sich handelt. Die korrekte
Lösung ist zu jedem Datenpunkt bereits im Voraus bekannt (Das Datenset ist
labeled). Falls die Beurteilung des Modelles nicht mit der bekannten, korrekten
Lösung übereinstimmt, passt sich das Modell automatisch auf eine bestimmte Weise
an. Dadurch soll die Beurteilungen für zukünftige Datenpunkte genauer werden.

Machine Learning Modelle unfassen verschiedene Hyperparameter. Diese beschreiben
unter anderem wie lange die Trainingsphase läuft oder wie stark sich das Modell
nach einer falschen Beurteilung anpasst. Diese Hyperparamter haben einen grossen
Einfluss auf das Lernverhalten des Modelles aber ihr optimaler Wert ist im
Voraus nicht bekannt. Hyperparamter können durch Algorithmen, wie den Baysian
Algorithmus optimiert werden.

Ein Machine Learning Modell kann also Daten Beurteilen und sich selbst Anpassen,
um die Beurteilungen zu verbessern. Künstliche Neuronale Netze umfassen diese
Funktionalität, und finden daher in Machine Learning Modellen Anwendung.


\subsection*{künstliche neuronale Netze}

Ein neuronales Netz ist, im biologischen Sinne, "eine beliebige Anzahl Neuronen,
die miteinander Verbunden sind" (Wikipedia). Ein Beispiel für ein neuronales
Netz ist das menschliche Gehirn. Künstliche Neuronale Netze modellieren
neuronale Netze in der Form von Programmcode. Diese Arbeit behandelt künstliche
neuronale Netze, nicht aber biologische. Somit handelt es sich bei jedem
erwähnten neuronalen Netz, um ein künstliches neuronales Netz.

Der Grundbaustein eines neuronalen Netzes ist das Neuron. Im Modell stellt
dieses ein Objekt dar, das eine beliebiege Anzahl Inputs, aber nur einen Output
hat. Input und Output sind hierbei rationale Zahlen. Die Ausgabe des Neurons ist
im einfachsten Modell, dem Perceptron, grundsätzlich entweder 0 oder 1. Die
Ausgabe ist 1, wenn die Summe der Eingaben einen vorgegebenen Wert, den
\emph{Bias} des Neurons überschreiten. Ansonsten ist die Ausgabe gleich 0. Jede
Eingabe hat ein \textbf{Gewicht}, das einer rationalen Zahl entspricht. Vor der
Addition der Inputs wird jeder Input mit seinem Gewicht multipliziert.  Die
Grösse des Gewichts bestimmt somit den Einfluss der zugehörigen Eingabe auf die
Ausgabe des Neurons. Neuronale Netze in Machine Learning Modellen verwenden
kompliziertere Neuronen als das Perceptron, wie zum Beispiel das Sigmoid-Neuron.
Der Hauptunterschied liegt dabei im Output. So kann im Sigmoid-Neuron der Output
zum Beispiel auch zwischen 0 und 1 sein.

Neuronale Netze sind Verbindungen dieser Neuronen. Dabei dient die Ausgabe eines
Neurons als eine Eingabe in ein anderes Neuron. Die Neuronen sind in
\emph{Layers} geordnet. Neuronale Netze haben mindestens eine \emph{Input Layer}
und eine \emph{Output Layer}. Die Input Layer umfasst die Daten, zu dem das
neuronale Netz eine Beurteilung liefert sollte. Im Beispielproblem (siehe
\nameref{chap:t_ml}) bestände die Eingabe-Ebene aus $28\times28$ Neuronen, wobei
jedes Neuron die Graustufe (durch einen Wert von 0 bis $255$) eines Pixels im
Bild beschreibt. Der Input ist in diesem Fall zweidimensional. Die Dimensionen
sind allerdings flexibel. Die Output layer besteht im Beispiel auf 10 Neuronen,
wobei jedes Neuron einer Beurteilung entspricht (das fünfte Neuron beschreibt
zum Beispiel die Beurteilung als eine Fünf). So entspricht dasjenige Neuron, das
den höchsten Output hat, der Beurteilung vom neuronalen Netz.

Zwischen der Input Layer und der Output Layers kann es weitere \emph{Hidden
Layers} geben. Es gibt verschiedene Arten von Hidden Layers, die verschidene
Funktionen haben. Zwei der meist verwendeten Layers sind Fully Connected (Dense)
Layers und Convolutional Layers. In Fully Connected Layers dient jedes Neuron
als Input für jedes Neuron in der nächsten Layer. In Convolutional Layers trifft
das nicht zu. Die Funktion von Convolutional Layers ist es, wichtige Merkmale
aus dem Input hervorzuheben. Machine Learning Modelle werden ab mehr als einer
Hidden Layer als Deep Learning Modelle bezeichnet.


Ein Machine Learning Modell passt während der Trainingsphase (siehe
\nameref{sub:funktionsweise}) einzelne Gewichte im neuronalen Netz in der
Hoffnung an, dass die Genauigkeit der Beurteilung mit den angepassten Gewichten
grösser ist. Die Genaue Anpassung erfolgt in den meisten Machine Learning
Modellen durch den Backpropagation Algorithmus



\section{Reinforcement Learning}
\label{chap:t_rl}
Reinforcement Learning bedeutet Lernen durch Interaktion mit der Umgebung.
(Sutten, Barto). Genauer gesagt soll ein Machine Learning Modell durch
Rückmeldungen aus einer Umgebung ein bestimmtes Verhalten erlernen.

Reinforcement Learning Modelle führen somit die Umgebung ein. Anders als bei
Supervised Learning und Unsupervised Learning sind die Daten, aus denen das
Modell lernen soll, im Voraus nicht bekannt. Das liegt in der Natur in der
Umgebung, die häufig zu viele verschiedene Zustände einnehmen kann, als dass
diese in einem Datenset gesammelt werden könnten. Ein Machine Learning Modell
kann trotzdem aus einer Umgebung lernen, indem es selbst mit dieser interagiert
und dadurch Erfahrungen sammelt. 

Die echte Welt kann ebenfalls als eine Umgebung angesehen werden. Der Mensch
wäre in diesem Fall der Machine learning Algorithmus. Der Mensch lernt durch
Interaktion mit seiner Umgebung die Eigenschaften dieser kennen. So lernt ein
Mensch die Schwerkraft durch das Hinfallen kennen. Durch diese Erfahrungen kann
der Mensch ein gewissen Verhalten, zum Beispiel das Laufen, erlernen.
Reinforcement Learning Modelle stellen dieses Lernverhalten nach. So verwendet
die Robotik häufig diesen Ansatz um einen Roboter laufen zu lassen. Die Umgebung
wodurch das reinforcement Learning Modell learnt ist dabei häufig nicht nicht
echte Welt, sondern eine simulierte Umgebung.


\subsection*{Aufbau und Funktionsweise}
Dieser Abschnitt umfasst eine genauere Erklärung eines Reinforcement Learning
Modelles (in diesem Fall Deep Q-Learning) unter der Verwendung der korrekten
Fachbegriffe.

% Todo: Umgebung -> Environment
Ein Reinforcement Learning Agent umfasst eine \emph{Umgebung} und einen
\emph{Agent}. Der Agent ist dasjenige Element in der Umgebung, welches mit
dieser interagiert und daraus lernt. Die Umgebung verändert sich in
Zeitschritten, genannt \emph{Steps}. In jedem Step führt der Agent eine
\emph{Action} aus, die die Umgebung beeinflusst. Die Entscheidung, welche Action
der Agent ausführt basiert auf einer \emph{Observation} der Umgebung. Die
Observation umfasst alle Daten der Umgebung, die für die Entscheidung des Agents
relevant sind. Die Entscheidung, welche Aktion der Agent ausführt, basiert auf
einem neuronalen Netz. Der Input in dieses neuronale Netz ist die Observation
der Umgebung und der Output beschreibt die action, die der Agent ausführt. Jedes
Neuron des Outputs beschreibt eine spezifische Action des Agents. Der Agent kann
somit nur eine feste Anzahl Actions ausführen. Diese Actions zusammen werden
\emph{Action-Space} genannt. Jede Action im Action-Space besitzt einen
\emph{Q-Value}, der dem Output des zugehörigen neurons entspricht. Die
Entscheidung, welche Action ausgeführt wird basiert auf der
\emph{Epsilon-Greedy} Strategie. Diese Strategie sieht vor, dass die
Entscheidung mit einer Wahrscheinlichkeit von $\epsilon$ auf eine zufällige
Action fällt. Ansonsten Fällt die Entscheidung auf diejenige Action mit dem
höchsten Q-Value. Die zufälligen Aktionen des Agents stellen eine Erkundung der
Umgebung dar.

Die Umgebung und somit auch der Agent werden durch die Actions des Agenten
beeinflusst. Dieser Einfluss wird durch die \emph{Reward-Function} gemessen. Die
Reward-Function gibt eine rationale Zahl, den \emph{Reward} aus. Umso grösser
der Reward, desto positiver ist der Effekt auf die Umgebung und umgekehrt. Ein
positiver Einfluss auf die Umgebung durch eine Action ist so definiert, dass der
Agent durch die Action das gewünschte Verhalten vorzeigt. Die Reward-Function
definiert, welches Verhalten welchen Reward erzielt. Der Q-Value der gewählten
Action wird mit dem Reward (und dem Maximalen Q-Value aus den nächsten möglichen
Actions) addiert. Diese Formel nennt sich Bellman-Gleichung. Der neue Q-Value          % Todo Ref Bellman Gleichung, Ref Sutton-Barto
nimmt somit einen kleineren Wert an wenn der Reward negativ ist und einen
grösseren Wert wenn der Reward positiv ist. Die Gewichte des neuronalen Netzes
werden nachfolgend so angepasst, dass der Output für das Neuron, dessen Action
ausgeführt wurde, näher am neu berechneten Q-Value ist. Der schlussendliche
Effekt ist, dass Actions, die in einem gewissen Zustand der Umgebung einen
positiven Reward auslösen wahrscheinlicher werden und umgekehrt Actions mit
negativen Rewards unwahrscheinlicher werden. Der Agent versucht insgesamt durch
seine Actions einen möglichst hohen akkumulierten reward zu erzielen. Der
akkumulierte Reward entsprich der Summe der Rewards aus jedem Step


Das Trainingsphase läuft in \emph{Episodes}. Eine Episode umfasst eine gewisse
Anzahl Steps und am Anfang jeder Episode wird die Umgebung in einen Startzustand
zurückgesetzt. Die Resultate eines Steps, also Die Zustände der Umgebung, die
Actions und der Reward, werden in dem \emph{Replay-Buffer} gespeichert. Dieser
Enthält Speicherplatz für eine bestimmte Anzahl Steps. Während dem Training
werden zufällige Steps aus dem Replay-Buffer gewählt, auf die das neuronale Netz
trainiert. Das neurnale Netz trainiert also auf Daten aus der Vergangenheit
der Umgebung und des Agents. Ausserdem trainiert das neuronale Netz jeweils mit
einem \emph{Batch} an Steps, also mit einer gewissen Anzahl an Steps
gleichzeitig. Der Replay-Buffer und der Batch sichern zu, dass das neuronale
Netz mit einer grossen Varianz an Steps trainiert, was das Lernverhalten
stabiler macht.



\section{Verwandte Arbeiten und Themen}
\label{chap:t_verwandt}
Es gibt verschiedene Ansätze, um ein Computerprogramm die menschliche Tätigkeit
des Nachzeichnens verrichten zu lassen. Diese Ansätze werden in verschiedenen
wissenschaftlichen Arbeiten verfolgt. Ein häufiger Ansatz ist ``Stroke-Based
Rendering'', wobei Bilder durch das Platzieren von Elementen wie Strichen
gezeichnet werden. Beispiele für Arbeiten, die diesen Ansatz verwenden sind\dots % TODO: Quelle anfügen
Stroke-Based Rendering unterscheidet sich von menschlichem Zeichnen dadurch,
dass kein Stift geführt wird. Stattdessen können die Elemente zu jedem Zeitpunkt
an einer willkürlichen Position auf der Zeichenfläche platziert werden.

Andere Ansätze simulieren die Führung eines Stiftes. Das Computerprogramm kann
also nicht zu jedem Zeitpunkt an jedem Ort Zeichnen. Stattdessen ist es an eine
Position (einen Stift) gebunden, welche mit einer gewissen Geschwindigkeit
bewegt werden kann. Das ist eine Einschränkung, die auch auf menschliches
Zeichnen mit einem Stift zutrifft. Ein Beipiel von diesem Programm ist Doodle-SDQ von Tao Zhou et Al (2018)

\subsection*{Doodle-SDQ}
Doodle-SDQ ist ein Programm, das durch Deep Q Learning erlernt hat, Strichbilder
aus dem Google Quick-Draw Datenset nachzuzeichnen. Das Programm stammt aus der
Arbeit `Learning to Doodle with Deep Q-Networks and Demonstration Strokes' von
Tao Zhou et Al. aus dem Jahr 2018. Wie der Titel beschreibt basiert das Programm
auf dem Deep Q-Learning Algorithmus (siehe \nameref{chap:t_rl}). `Demonstration
Strokes' ist ein weiteres verfolgtes Konzept in der Arbeit. Dieses gehört
allerdings nicht zum Bereich von Reinforcement learning, sondern ist vereinfacht
gesagt eine Methode zur Optimierung des Algorithmus. Die Folgende Erklärung
bezieht sich daher nur auf den Deep Q-Learning Algorithmus.

Das Quick-Draw Datenset von Google beinhaltet Bilder von verschiedenen,
menschlichen Strichzeichnungen. Das Programm versucht, diese Bilder
nach\hyp{}zuzeichnen. Dazu werden die Bilder zuerst auf eine einheitliche Grösse von
$84\times84$ Pixeln komprimiert. Der Agent kann sich auf dieser Fläche bewegen und
Zeichnen. Es handelt sich bei dieser Zeichenfläche um die Umgebung des Agenten
Der Agent kann sich mit einer Aktion auf einen beliebigen Pixel in einem $11\times11$
Feld, in dessen Zentrum er ist, bewegen. Dabei kann der Agent entweder zwischen
seiner alten und seiner neuen Position Zeichnen oder nicht. Der Agent kann somit
insgesamt $2\times11\times11$ Aktionen ausführen. Diese Anzahl Aktionen entspricht
gleichzeitig der Anzahl Neuronen in der Ausgabe des neuronalen Netzes. Die
Eingabe in das Netz, also die Beobachtung der Umgebung wird in zwei Teile
gegliedert: Ein globaler Teil (global channel) und ein lokaler Teil (local
channel). Der Globale Teil umfasst das vorgegebene Bild (die Vorlage), das bis
anhin gezeichnete Bild, die aktuelle Position des Agenten und die Angabe, ob der
Agent gerade am Zeichnen ist oder nicht. Der Lokale Teil umfasst nur noch das
$11\times11$ Feld vom vorgegebenen und dem bis anhin gezeichneten Bild, das sich direkt
um den Agenten befindet. Diese erneute Eingabe des kleineren Feldes in das Netz
ist notwendig, weil die unmittelbare Umgebung des Agenten für die Entscheidung
der nächsten Aktion sehr relevant ist. Der Reward wird durch die Anzahl der neu
übereinstimmenden Pixeln zwischen der Vorlage und dem bis anhin
gezeichneten Bild bestimmt.

\section{Git und GitHub}
\label{chap:git_github}
Git und Github sind weit verbreitete Hilfsmittel für Entwickler. Bei Git handelt
es sich um ein Programm, während Github ein Service ist, um Projekte, welche mit
Git verwaltet werden, in der Cloud zugänglich zu machen. GitHub bringt zusätzlich
viele weitere nützliche Features, welche die Zusam\hyp{}menarbeit erleichtern. Die
genaue Funktion und das Zusammenspiel dieser beiden Hilfsmittel wird nachfolgend
erläutert.

\subsection*{Git}
Git ist ein Programm, welches Veränderungen im Code eines Projektes erkennt und
zwischen Versionen speichert. Dieses Konzept nennt sich Version Control. Es
wurde 2005 von Linus Torvald für die Entwicklung des Linux Kernels entwickelt.
Der Unterschied zwischen Git und anderer Version Control Software ist, dass
jeder, der am Code arbeiten will, den gesamten Code, auch Source Code genannt,
braucht. Dadurch ist das System dezen\hyp{}tralisierter.
\cite{noauthor_git_2021}

Git ist nicht die einzige Version Control Software. Andere Beispiele wären:
Azure DevOps Server, Helix Core, AWS CodeCommit, Subversion, Plastic SCM, etc. 
\cite{noauthor_git_nodate-1}

\subsection*{GitHub}
Es wird häufig gedacht, dass Git und GitHub dasselbe sei, was allerdings nicht
richtig ist. Git ist die Version Control Software, die das Verwalten der
verschiedenen Versionen ermöglicht. GitHub ist ein Service, der es ermöglicht,
die von Git erstellten Repositories als Cloud Lösung bereit zu stellen. Durch
diesen Cloud Service von GitHub ist es möglich, gemeinsam an den verschiedenen
Versionen des Source Codes zu arbeiten. GitHub stellt dafür auch eine Webseite
zur Verfügung, um die Projekte zu verwalten. Dabei ist Git für die Bearbeitung
der Repositiories zuständig, während GitHub die Repositories den anderen
Entwicklern bereitstellt.
\cite{noauthor_github_2021} 

``GitHub wurde von Chris Wanstrath, PJ Hyett, Scott Chacon und Tom Preston-Werner
[\ldots] entwickelt und im Februar 2008 gestartet'' \cite{noauthor_github_2021}.
2018 wurde das Unternehmen von Microsoft gekauft. GitHub ist heute eine der
grössten Plattformen für Open Source Projekte und das Zusammenarbeiten an
Software. Projekte können ohne Umstände für Leute auf der ganzen Welt zugänglich
gemacht werden. Wenn Entwickler an einem öffentlichen Projekt auf Github
Interesse finden, können sie selbst daran weiterarbeiten. 
\cite{noauthor_github_2021}

Um die Codequalität trotz einer grossen Menge an alleinstehenden Ent\hyp{}wicklern zu
gewährleisten, gibt es auf GitHub einige Tools, die das Zusamm\hyp{}enarbeiten, das
Kommunizieren und das Bewerten von Code vereinfachen.

GitHub ist nicht die einzige Plattform, welche das Hosten von Git Repositories
erlaubt. Es gibt weitere Konkurrenten, wie: GitLab, Bitbucket, GitBucket, etc.
\cite{noauthor_top_2021}
