\chapter{Theoretische Grundlagen}

\section{Machine Learning}
\label{chap:t_ml}
Teilbereich künstliche Intelligenz (Künstliche Intelligenz ist Maschine, die
menschliche Tätigkeiten nachmacht) Maschine = Computer -> Computerprogramm
(Algorithmus) Machine Learning ist Überbegriff -> Computerprogramme, die eine
Mustererkennung entwickeln. Durch Analysieren von Daten. KI lernt Vorhersagen

Beispiel Zahlenerkennung: Beispiel für Künstliche Intelligenz Bild einer
handschriftlichen Zahl wird Programm übergeben (Input) Das Programm bestimmt, um
welche Zahl es sich handelt. Programm gibt Zahl aus

Künstliche Intelligenz weil das Erkennen von Zahlen menschlich ist.

Unterschied machine Learning: KI lernt von Grund auf, Zahlen zu Erkennen Anfang
wird allermeistens Falsch sein

KI lernt durch die Analyse von Daten. -> Trainingsdaten. Dabei ist bekannt, was
die richtige Lösung zu den gegebenen Daten ist. So erfährt die KI, wenn sie
falsch liegt. Programm kann sich bezogen auf Erfahrung selbst anpassen.
Anpassungen möglichst so, dass Vorhersagen in Zukunft besser sind.

Nachdem KI mit Trainingsdaten verbessert wurde, sollte diese auch für neue Daten
möglichst genaue Vorhersagen machen.

Nächste Frage: Wie trifft KI entscheidungen, und wie kann sie sich selbst
Anpassen? -> Antwort Neuronale Netze


\subsection*{künstliche neuronale Netze}

Grundbaustein eines neuronalen Netzes sind Neuronen, (modelle eines biologischen
Neurons) mehrere Eingaben, eine Ausgabe. Eingaben sind verschieden wichtig (sind
gewichtet) Wenn Eingaben zusammen einen gewissen Grenzwert überschreiten,
"Feuert" das Neuron. 1 anstelle von 0 wenn es nicht feuert (vereinfachte
Erklärung eines Sigmoid-Neuron)

Es gibt Neuronen mit Eingaben in KI. Diese haben verbindungen zu weiteren
Neuronen in der nächsten Ebene (versteckte Ebene). Die Letzte Ebene des Netzes
hat die Ausgabe Neuronen (mit MNIST Beispiel erklären. Im Beispiel sind es 10
Neuronen, wobei jedes Neuron eine andere Zahl von 0-9 darstellt. Diese Neuronen
haben keine Ausgabe mehr, die für das neuronale Netz relevant ist. Die
Entscheidung basiert in diesem Fall lediglich darauf, welches der 10
Ausgabeneurenen den höchsten Wert enthält. Die Zahl, die dieses Neuron
repräsentiert, wird dann als die Entscheidung des Neuronalen Netzes angesehen).
mehr als eine versteckte Ebene ermöglicht schwierigere Entscheidungen -> Deep
learning

Der Lernprozess findet durch eine anpassung von Einzelnen Gewichten der
Eingaben, wenn die KI eine falsche Entscheidung trifft. Dadurch soll die
Entscheidung so angepasst werden, dass sie im nächsten Fall besser ausfällt

Neuronale Netze sind in Ebenen aufgeteilt. Eingabe Ebene, Ausgabe Ebene. Und
Ebenen dazwischen. Versteckte Ebenen. Sobald es mehr als eine Ebene an
versteckten Ebene gibt, nennt man es Deep learning.

\subsection*{Arten von Machine Learning}

Es gibt 3 Arten von Machine Learning Algorithmen: supervised, unsupervised, reinforcement
learning. Nachfolgend werden supervised und unsupervised learning kurz erklärt.
Reinforcement learning im nächsten Kapitel, weil für Methode hinter Arbeit
wichtig

Erkennung von Handschriftlichen Daten ist supervised learning ->
Typisch sind Trainingsdaten, bei denen die richtige Lösung bekannt ist.
Algorithmus wird also auf Vorhersagen trainiert, die vorher als richtig
angesehen wurden. Gewissermassen also Algorithmus in Freiheit seiner Lösung
eingeschränkt und passt sich den Vorstellungen von demjenigen an, der die Daten präpariert hat (Also die seiner Meinung nach richtigen Lösungen) an. Wird also überwacht.

unsupervised learning ist das Gegenteil. Wird nicht auf vorgegebene Lösungen
trainiert. Stattdessen soll Algorithmus selbstständig Muster in Daten erkennen.
Es wird also im voraus nicht definiert, was der Algorithmus lernen soll.
Angewendet für Gliederungen und Klassifikationen in Datensätzen, wo Menschen den
Überblick verlieren würden. Ausserdem, weil im voraus komplett unklar ist, wie
der Algorithmus die Daten verarbeitet, können neue Muster erkannt werden.
Muster, die von Menschen bisher übersehen wurden 


\section{Reinforcement Learning}
\label{chap:t_rl}

Reinforcement Learning bedeutet vereinfacht Lernen durch Interaktion mit der
Umgebung. (Sutten, Barto)  Genauer lernt ein Machine Learning Algorithmus durch
die Interaktion mit einer Umgebung, wie er sich in dieser Verhalten soll.

Reinforcement Learning Algorithmen führen also die Umgebung ein. In Supervised
Learning und Unsupervised Learning werden Daten bereitgestellt, aus denen der
Machine Learning Algorithmus "lernt". Bei einer Umgebung sind diese Daten
allerdings im Voraus nicht bekannt. Das kommt daher, dass eine Umgebung häufig
zu viele verschiedene Zustände einnehmen kann, als dass diese im Vorraus erfasst
werden könnten. Der Machine Learning Algorithmus kann trotzdem aus der Umgebung
lernen, indem dieser selbt als Element der Umgebung angesehen wird und so mit ihr interagieren kann.


Die echte Welt kann ebenfalls als eine Umgebung angesehen werden. Der Mensch
wäre in diesem Fall der Machine learning Algorithmus. Der Mensch erlernt zum
Beispiel das Laufen, indem er mit der Welt interagiert. Eine Interaktion wäre
hier zum Beispiel das Hinfallen. Dadurch erlernt der Mensch das Verhalten der
Schwerkraft, welches zu diesem Fall geführt hat. Der Mensch lernt dieses
Verhalten durch eigene Erfahrungen, ohne dass ihm vorher bereits Daten über die
Welt gezeigt wurden. 

Reinforcement Learning Algorithmen stellen dieses Lernverhalten nach. So
verwenden Roboter, die das Laufen lernen sollen häufig diesen Ansatz. Die
Umgebung ist für den Reinforcement Learning Algorithmus im Roboter tatsächlich
die echte Welt. Bei der Umgebung kann es sich aber auch um eine
Computersimulation handeln.

\subsection*{Aufbau und Funktionsweise}

Dieser Abschnitt umfasst eine genauere Erklärung eines Reinforcement Learning
Algorithmus (Deep Q-Learning) unter der Verwendung der korrekten Fachbegriffe.

Ein Reinforcement Learning Algorithmus umfasst eine Umgebung und einen Agenten.
Der Agent ist dasjenige Element in der Umgebung, welches mit dieser interagiert
und daraus lernt. Der Agent umfasst ein Neuronales Netz. Die Eingabe in dieses
Netz ist die \textbf{Beobachtung} der Umgebung, also diejenigen aktuellen Daten
aus der Umgebung, die für den Agenten relevant sind. Die Ausgabe des Netzes
entspricht einer gewissen Anzahl Neuronen, die alle einen bestimmten Wert (den
\textbf{Q-Wert}) haben, der jeweils von der Eingabe abhängt. Jedes Neuron
repräsentiert eine Aktion, wobei jenes Neuron mit dem höchsten Q-Wert als die
`beste' Aktion angesehen wird. Der Agent kann somit nur eine festgelegte Anzahl
verschiedener Aktionen ausführen, die durch die Anzahl der Neuronen der Ausgabe
definiert ist. Der Agent führt in den meisten Fällen diejenige Aktion aus, die
als die beste definiert ist. Mit einer kleinen Wahrscheinlichkeit führt er
allerdings eine zufällige Aktion aus. Die Hoffnung dahinter ist, dass sich die
zufällige Aktion schlussendlich als noch besser herausstellt 

Nachdem der Agent eine Aktion ausführt, wird der Einfluss dieser Aktion auf die
Umgebung gemessen. Dabei bestimmt eine \textbf{Reward-funciton}, ob die Aktion
positiv oder negativ auf die Umgebung wirkt. Das wird durch eine Zahl
ausgedrückt. Umso grösser die Zahl, desto positiver ist der Effekt auf die
Umgebung und umgekehrt. Diese Zahl wird Belohnung oder \textbf{Reward} genannt.
Der Q-Wert der ausgeführten Aktion wird schliesslich bezogen auf den Reward
angepasst (Durch die Bellman Gleichung). Ein Kleinerer Reward führt zu einem
kleineren Q-Wert für die Aktion, wodurch diese in Zukunft weniger wahrscheinlich
gewählt wird. Umgekehrt macht ein grösserer Reward die Aktion, die diesen
ausgelöst hat, in Zukunft wahrscheinlicher. Nach der Anpassung des Q-Werts wird
das Neuronale Netz auf diese neue Ausgabe trainiert. Die Gewichte im Neuronalen
Netz werden also so angepasst, dass die Ausgabe einer ähnliche Eingabe in
Zukunft näher an den neu angepassten Q-Werten ist als an den alten.





\section{Verwandte Arbeiten und Themen}
\label{chap:t_verwandt}
Es gibt verschiedene Ansätze, um ein Computerprogramm die menschliche Tätigkeit
des Nachzeichnens verrichten zu lassen. Diese Ansätze werden in verschiedenen
wissenschaftlichen Arbeiten verfolgt. Ein häufiger Ansatz ist "Stroke-Based
Rendering", wobei Bilder durch das Platzieren von Elementen wie Strichen
gezeichnet werden. Beispiele für Arbeiten, die diesen Ansatz verwenden sind\dots
Stroke-Based Rendering unterscheidet sich von menschlichem Zeichnen dadurch,
dass kein Stift geführt wird. Stattdessen können die Elemente zu jedem Zeitpunkt
an einer willkürlichen Position auf der Zeichenfläche platziert werden.

Andere Ansätze simulieren die Führung eines Stiftes. Das Computerprogramm kann
also nicht zu jedem Zeitpunkt an jedem Ort Zeichnen. Stattdessen ist es an eine
Position (einen Stift) gebunden, welche mit einer gewissen Geschwindigkeit
bewegt werden kann. Das ist eine Einschränkung, die auch auf menschliches
Zeichnen mit einem Stift zutrifft. Ein Beipiel für ein Computerprogramm, das diese Art des Zeichnens Nachahmt ist
Doodle-SDQ von ...


\subsection*{Doodle-SDQ}
Doodle-SDQ ist ein Programm, das durch Deep Q Learning erlernt hat, Strichbilder
aus dem Google Quick-Draw Datenset nachzuzeichnen. Ist in der Freiheit des
Zeichnens eingeschränk\indent 

Architektur: Deep Q-Learning Algorithmus.
Agent = Stift. Umgebung = Zeichenfläche (Schwarz Weiss). Speichert Position von
Agent, das zu Nachzeichnende Bild, das was bis jezt gezeichnet wurde und ob
Stift gerade vom Blatt gehoben ist. Die Belohnung/Bestrafung ist der Grad der
Ähnlichkeit zwischen Bildern. Die Unmittelbare Umgebung des Agenten, in dem er
sich in einem Schritt bewegen kann, wird in das neuronale Netz noch ein weiteres
Mal eingegeben, wodurch darauf ein Fokus gelegt wird. Ausgabe des neuronalen
Netz ist eine der Aktionen die der Agent ausführen kann.



\section{Git und GitHub}
\label{chap:t_git}



