{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tic Tac Toe\n",
    "Ein simpler Test von RL mit einem 2D Environnement, wobei die States 2D sind. Das würde für unser Drawing System ein einfachreres System ermöglichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.environments import utils\n",
    "\n",
    "import reverb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tic Tac Toe Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(py_environment.PyEnvironment):\n",
    "    def __init__(self):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(), dtype=np.int32, minimum=0, maximum=8, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(9,), dtype=np.int32, name='observation')\n",
    "        self._state = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        self.status = np.array(self._state).reshape(3, 3)\n",
    "        self._episode_ended = False\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        self.status = np.array(self._state).reshape(3, 3)\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(np.array(self._state, dtype=np.int32))\n",
    "\n",
    "    def _step(self, action):\n",
    "\n",
    "        self.status = np.array(self._state).reshape(3, 3)\n",
    "\n",
    "        if self._episode_ended:\n",
    "            # The last action ended the episode. Ignore the current action and start\n",
    "             # a new episode.\n",
    "            return self.reset()\n",
    "\n",
    "        reward = 0.0\n",
    "        #\n",
    "        # Tic Tac Toe Logic\n",
    "        #\n",
    "        \n",
    "        y = action // 3\n",
    "        x = action % 3\n",
    "        if self.status[y][x] > 0:\n",
    "            reward -= 1\n",
    "        else:\n",
    "            self.status[y][x] = 1\n",
    "        \n",
    "        full = True\n",
    "        for i in self.status:\n",
    "            for e in i:\n",
    "                if e == 0:\n",
    "                    full = False\n",
    "        \n",
    "        win = [False, False]\n",
    "        for i in range(1, 3):\n",
    "            # Row\n",
    "            for e in self.status:\n",
    "                if e[0] == i and e[1] == i and e[2] == i:\n",
    "                    win[i-1] = True\n",
    "            \n",
    "            # Column\n",
    "            for e in range(3):\n",
    "                if self.status[0][e] == i and self.status[1][e] == i and self.status[2][e] == i:\n",
    "                    win[i-1] = True\n",
    "            \n",
    "            # Diagonal\n",
    "            if self.status[0][0] == i and self.status[1][1] == i and self.status[2][2] == i:\n",
    "                win[i-1] = True\n",
    "                \n",
    "            # Anti Diagonal\n",
    "            if self.status[0][2] == i and self.status[1][1] == i and self.status[2][0] == i:\n",
    "                win[i-1] = True\n",
    "        \n",
    "        if full:\n",
    "            self._episode_ended = True\n",
    "        if win[0]:\n",
    "            self._episode_ended = True\n",
    "            reward += 20\n",
    "        if win[1]:\n",
    "            self._episode_ended = True\n",
    "            reward -= 20\n",
    "        \n",
    "        if self._episode_ended:\n",
    "            return ts.termination(np.array(self._state, dtype=np.int32), reward)\n",
    "            \n",
    "        while True:\n",
    "            x = random.randint(0, 2)\n",
    "            y = random.randint(0, 2)\n",
    "            if self.status[y][x] == 0:\n",
    "                self.status[y][x] = 2\n",
    "                break\n",
    "\n",
    "        self._state = list(self.status.reshape(9,))\n",
    "        \n",
    "        return ts.transition(np.array(self._state, dtype=np.int32), reward=reward, discount=1.0)\n",
    "    \n",
    "    def render(self):\n",
    "        for i in self.status:\n",
    "            row = \"\"\n",
    "            for e in i:\n",
    "                row += str(e) + \"|\"\n",
    "            print(row)\n",
    "            print(\"-|-|-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_py = Game()\n",
    "env = tf_py_environment.TFPyEnvironment(env_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0|0|0|\n",
      "-|-|-\n",
      "0|0|0|\n",
      "-|-|-\n",
      "0|0|0|\n",
      "-|-|-\n"
     ]
    }
   ],
   "source": [
    "env_py.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': array(1., dtype=float32),\n",
       " 'observation': array([0, 2, 1, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 'reward': array(0., dtype=float32),\n",
       " 'step_type': array(1, dtype=int32)})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestep = env_py._step(2)\n",
    "timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestep.is_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 1, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_py._state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': array(1., dtype=float32),\n",
       " 'observation': array([0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 'reward': array(0., dtype=float32),\n",
       " 'step_type': array(0, dtype=int32)})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_py.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Env Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(8, dtype=int32)),\n",
       " BoundedTensorSpec(shape=(9,), dtype=tf.int32, name='observation', minimum=array(-2147483648, dtype=int32), maximum=array(2147483647, dtype=int32)),\n",
       " TimeStep(\n",
       " {'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       "  'observation': BoundedTensorSpec(shape=(9,), dtype=tf.int32, name='observation', minimum=array(-2147483648, dtype=int32), maximum=array(2147483647, dtype=int32)),\n",
       "  'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       "  'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_spec(), env.observation_spec(), env.time_step_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 16:55:27.135298: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: UNKNOWN ERROR (100)\n",
      "2022-04-17 16:55:27.135348: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-ORA727V): /proc/driver/nvidia/version does not exist\n",
      "2022-04-17 16:55:27.143500: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
       " 'observation': <tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,\n",
       " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
       " 'observation': <tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[0, 2, 0, 0, 0, 0, 1, 0, 0]], dtype=int32)>,\n",
       " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._step(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': BoundedTensorSpec(shape=(9,), dtype=tf.int32, name='observation', minimum=array(-2147483648, dtype=int32), maximum=array(2147483647, dtype=int32)),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.time_step_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Agent Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Actions:  9\n"
     ]
    }
   ],
   "source": [
    "fc_layer_params = (100, 50) # Die Anzahl der Dense Units in einem Layer\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "print(\"Num Actions: \", num_actions)\n",
    "\n",
    "# Helper function um die Dense Layer zu kreeieren.\n",
    "def dense_layer(num_units):\n",
    "  return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# Create the dense layer array\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "\n",
    "# Create the output layer\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "\n",
    "# Sequentialize the layers into a tensorflow model\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    env.time_step_spec(),\n",
    "    env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "            total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-19.116"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return(env,\n",
    "                   random_tf_policy.RandomTFPolicy(env.time_step_spec(),\n",
    "                                                   env.action_spec()),\n",
    "                   1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/platform/tfrecord_checkpointer.cc:150]  Initializing TFRecordCheckpointer in /tmp/tmpqn1jbs0s.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:386] Loading latest checkpoint from /tmp/tmpqn1jbs0s\n",
      "[reverb/cc/platform/default/server.cc:71] Started replay server on port 22651\n"
     ]
    }
   ],
   "source": [
    "table_name = 'uniform_table'\n",
    "replay_buffer_signature = tensor_spec.from_spec(\n",
    "      agent.collect_data_spec)\n",
    "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
    "    replay_buffer_signature)\n",
    "\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=100000,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "    signature=replay_buffer_signature)\n",
    "\n",
    "reverb_server = reverb.Server([table])\n",
    "\n",
    "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    table_name=table_name,\n",
    "    sequence_length=2,\n",
    "    local_server=reverb_server)\n",
    "\n",
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "    replay_buffer.py_client,\n",
    "    table_name,\n",
    "    sequence_length=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TimeStep(\n",
       " {'discount': array(1., dtype=float32),\n",
       "  'observation': array([0, 1, 2, 0, 0, 0, 1, 0, 2], dtype=int32),\n",
       "  'reward': array(0., dtype=float32),\n",
       "  'step_type': array(1, dtype=int32)}),\n",
       " ())"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_driver.PyDriver(\n",
    "    env_py,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "        random_tf_policy.RandomTFPolicy(env.time_step_spec(),\n",
    "                                        env.action_spec()),\n",
    "        use_tf_function=True),\n",
    "        [rb_observer],\n",
    "        max_steps=100\n",
    "    ).run(env_py.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (3631) so Table uniform_table is accessed directly without gRPC.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Trajectory(\n",
       " {'action': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 5], dtype=int32)>,\n",
       "  'discount': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>,\n",
       "  'next_step_type': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 1], dtype=int32)>,\n",
       "  'observation': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 2, 0, 0, 0]], dtype=int32)>,\n",
       "  'policy_info': (),\n",
       "  'reward': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 0., -1.], dtype=float32)>,\n",
       "  'step_type': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1], dtype=int32)>}),\n",
       " SampleInfo(key=<tf.Tensor: shape=(2,), dtype=uint64, numpy=array([12835331676194504638, 12835331676194504638], dtype=uint64)>, probability=<tf.Tensor: shape=(2,), dtype=float64, numpy=array([0.01020408, 0.01020408])>, table_size=<tf.Tensor: shape=(2,), dtype=int64, numpy=array([98, 98])>, priority=<tf.Tensor: shape=(2,), dtype=float64, numpy=array([1., 1.])>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(replay_buffer.as_dataset()).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2).prefetch(3)\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f7bc44fa400>\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (3631) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (3631) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (3631) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (3631) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (3631) so Table uniform_table is accessed directly without gRPC.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Trajectory(\n",
       " {'action': <tf.Tensor: shape=(64, 2), dtype=int32, numpy=\n",
       " array([[8, 7],\n",
       "        [2, 6],\n",
       "        [0, 8],\n",
       "        [0, 3],\n",
       "        [4, 8],\n",
       "        [6, 0],\n",
       "        [5, 5],\n",
       "        [8, 6],\n",
       "        [1, 1],\n",
       "        [0, 2],\n",
       "        [8, 0],\n",
       "        [1, 3],\n",
       "        [5, 0],\n",
       "        [8, 6],\n",
       "        [1, 0],\n",
       "        [5, 7],\n",
       "        [7, 4],\n",
       "        [6, 0],\n",
       "        [1, 3],\n",
       "        [1, 4],\n",
       "        [6, 1],\n",
       "        [4, 5],\n",
       "        [6, 2],\n",
       "        [7, 2],\n",
       "        [3, 4],\n",
       "        [4, 0],\n",
       "        [6, 6],\n",
       "        [3, 3],\n",
       "        [4, 1],\n",
       "        [3, 2],\n",
       "        [0, 5],\n",
       "        [3, 5],\n",
       "        [3, 0],\n",
       "        [5, 3],\n",
       "        [7, 2],\n",
       "        [0, 5],\n",
       "        [7, 3],\n",
       "        [6, 7],\n",
       "        [2, 2],\n",
       "        [6, 2],\n",
       "        [8, 0],\n",
       "        [3, 2],\n",
       "        [1, 8],\n",
       "        [2, 2],\n",
       "        [7, 2],\n",
       "        [3, 5],\n",
       "        [6, 0],\n",
       "        [4, 4],\n",
       "        [1, 8],\n",
       "        [4, 6],\n",
       "        [1, 1],\n",
       "        [8, 6],\n",
       "        [8, 6],\n",
       "        [6, 6],\n",
       "        [2, 5],\n",
       "        [2, 6],\n",
       "        [6, 2],\n",
       "        [6, 1],\n",
       "        [1, 4],\n",
       "        [1, 4],\n",
       "        [2, 5],\n",
       "        [5, 4],\n",
       "        [2, 0],\n",
       "        [4, 5]], dtype=int32)>,\n",
       "  'discount': <tf.Tensor: shape=(64, 2), dtype=float32, numpy=\n",
       " array([[0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]], dtype=float32)>,\n",
       "  'next_step_type': <tf.Tensor: shape=(64, 2), dtype=int32, numpy=\n",
       " array([[2, 0],\n",
       "        [1, 2],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [2, 0],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [1, 1],\n",
       "        [2, 0],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [1, 2],\n",
       "        [2, 0],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [2, 0],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [2, 0],\n",
       "        [1, 2],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [1, 2],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [2, 0],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1]], dtype=int32)>,\n",
       "  'observation': <tf.Tensor: shape=(64, 2, 9), dtype=int32, numpy=\n",
       " array([[[2, 0, 1, ..., 1, 2, 2],\n",
       "         [2, 0, 1, ..., 1, 2, 2]],\n",
       " \n",
       "        [[2, 1, 1, ..., 1, 2, 0],\n",
       "         [2, 1, 1, ..., 1, 2, 2]],\n",
       " \n",
       "        [[2, 1, 0, ..., 0, 0, 0],\n",
       "         [2, 1, 0, ..., 2, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 2, 0, 0]],\n",
       " \n",
       "        [[0, 2, 0, ..., 0, 0, 0],\n",
       "         [0, 2, 1, ..., 2, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]], dtype=int32)>,\n",
       "  'policy_info': (),\n",
       "  'reward': <tf.Tensor: shape=(64, 2), dtype=float32, numpy=\n",
       " array([[-21.,   0.],\n",
       "        [ -1., -21.],\n",
       "        [ -1.,   0.],\n",
       "        [ -1.,  -1.],\n",
       "        [  0.,   0.],\n",
       "        [ -1., -21.],\n",
       "        [  0.,  -1.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,  -1.],\n",
       "        [ -1.,  -1.],\n",
       "        [ 20.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [ -1.,  -1.],\n",
       "        [ -1.,   0.],\n",
       "        [  0.,  -1.],\n",
       "        [ -1., -21.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [ -1., -21.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0., -21.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [ -1.,   0.],\n",
       "        [ -1.,  -1.],\n",
       "        [  0.,  -1.],\n",
       "        [  0.,  -1.],\n",
       "        [  0.,   0.],\n",
       "        [ -1., -21.],\n",
       "        [  0.,  -1.],\n",
       "        [ -1., -20.],\n",
       "        [  0., -21.],\n",
       "        [-20.,   0.],\n",
       "        [ -1.,   0.],\n",
       "        [  0.,  -1.],\n",
       "        [  0.,   0.],\n",
       "        [-21.,   0.],\n",
       "        [  0.,  -1.],\n",
       "        [  0.,   0.],\n",
       "        [ 20.,   0.],\n",
       "        [ -1., -21.],\n",
       "        [ -1.,  -1.],\n",
       "        [  0.,  -1.],\n",
       "        [ -1.,   0.],\n",
       "        [ -1., -20.],\n",
       "        [ -1., -21.],\n",
       "        [  0.,  -1.],\n",
       "        [ -1.,  -1.],\n",
       "        [-21.,   0.],\n",
       "        [  0.,  -1.],\n",
       "        [  0.,   0.],\n",
       "        [ -1.,   0.],\n",
       "        [  0.,  -1.],\n",
       "        [ -1.,  -1.],\n",
       "        [ -1., -21.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [ -1.,  -1.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,  -1.]], dtype=float32)>,\n",
       "  'step_type': <tf.Tensor: shape=(64, 2), dtype=int32, numpy=\n",
       " array([[1, 2],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [0, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [0, 1],\n",
       "        [1, 1],\n",
       "        [0, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [0, 1],\n",
       "        [1, 1],\n",
       "        [0, 1],\n",
       "        [1, 2],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [1, 1],\n",
       "        [0, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [0, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [0, 1],\n",
       "        [1, 1],\n",
       "        [0, 1]], dtype=int32)>}),\n",
       " SampleInfo(key=<tf.Tensor: shape=(64, 2), dtype=uint64, numpy=\n",
       " array([[ 2374869300629290173,  2374869300629290173],\n",
       "        [15696427652690825051, 15696427652690825051],\n",
       "        [ 6683615721871301268,  6683615721871301268],\n",
       "        [ 4177341615757293580,  4177341615757293580],\n",
       "        [16349863418756794363, 16349863418756794363],\n",
       "        [ 2321885736828617885,  2321885736828617885],\n",
       "        [12829575403664081317, 12829575403664081317],\n",
       "        [ 6938974452392001705,  6938974452392001705],\n",
       "        [14725914989976923920, 14725914989976923920],\n",
       "        [ 9346042916241222537,  9346042916241222537],\n",
       "        [14216553359236128777, 14216553359236128777],\n",
       "        [ 4545816593294929361,  4545816593294929361],\n",
       "        [17402333592716718686, 17402333592716718686],\n",
       "        [ 6370635413976326826,  6370635413976326826],\n",
       "        [13237805028775420826, 13237805028775420826],\n",
       "        [10417807537616208968, 10417807537616208968],\n",
       "        [14063003216944947049, 14063003216944947049],\n",
       "        [11306833523155245906, 11306833523155245906],\n",
       "        [ 9494351932044435238,  9494351932044435238],\n",
       "        [10521706176212783165, 10521706176212783165],\n",
       "        [ 9806469262074259500,  9806469262074259500],\n",
       "        [17637814742936074080, 17637814742936074080],\n",
       "        [11158763945517071053, 11158763945517071053],\n",
       "        [ 9078383915444423757,  9078383915444423757],\n",
       "        [ 2024622021128440175,  2024622021128440175],\n",
       "        [ 6002637912243357728,  6002637912243357728],\n",
       "        [16088581120928587003, 16088581120928587003],\n",
       "        [16357677585034559001, 16357677585034559001],\n",
       "        [12180632406343035248, 12180632406343035248],\n",
       "        [10928466644721944210, 10928466644721944210],\n",
       "        [  611849108535796415,   611849108535796415],\n",
       "        [ 4004667439664631435,  4004667439664631435],\n",
       "        [15871014578032803394, 15871014578032803394],\n",
       "        [ 7546738396358909724,  7546738396358909724],\n",
       "        [16489511054707881049, 16489511054707881049],\n",
       "        [  611849108535796415,   611849108535796415],\n",
       "        [15182352018773856488, 15182352018773856488],\n",
       "        [  468853593746471012,   468853593746471012],\n",
       "        [ 4758734924606201917,  4758734924606201917],\n",
       "        [ 8461539195412800265,  8461539195412800265],\n",
       "        [14216553359236128777, 14216553359236128777],\n",
       "        [ 1280480436244947644,  1280480436244947644],\n",
       "        [ 4754189789482646892,  4754189789482646892],\n",
       "        [ 4758734924606201917,  4758734924606201917],\n",
       "        [16489511054707881049, 16489511054707881049],\n",
       "        [ 4004667439664631435,  4004667439664631435],\n",
       "        [ 2321885736828617885,  2321885736828617885],\n",
       "        [ 7240521424908352084,  7240521424908352084],\n",
       "        [ 4754189789482646892,  4754189789482646892],\n",
       "        [11318150390690418579, 11318150390690418579],\n",
       "        [14725914989976923920, 14725914989976923920],\n",
       "        [ 6938974452392001705,  6938974452392001705],\n",
       "        [ 6370635413976326826,  6370635413976326826],\n",
       "        [16088581120928587003, 16088581120928587003],\n",
       "        [10517459061777839352, 10517459061777839352],\n",
       "        [15696427652690825051, 15696427652690825051],\n",
       "        [ 8461539195412800265,  8461539195412800265],\n",
       "        [ 9806469262074259500,  9806469262074259500],\n",
       "        [10521706176212783165, 10521706176212783165],\n",
       "        [10521706176212783165, 10521706176212783165],\n",
       "        [10517459061777839352, 10517459061777839352],\n",
       "        [ 6080001791839566004,  6080001791839566004],\n",
       "        [ 2110304805660772272,  2110304805660772272],\n",
       "        [12835331676194504638, 12835331676194504638]], dtype=uint64)>, probability=<tf.Tensor: shape=(64, 2), dtype=float64, numpy=\n",
       " array([[0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408],\n",
       "        [0.01020408, 0.01020408]])>, table_size=<tf.Tensor: shape=(64, 2), dtype=int64, numpy=\n",
       " array([[98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98],\n",
       "        [98, 98]])>, priority=<tf.Tensor: shape=(64, 2), dtype=float64, numpy=\n",
       " array([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int32, numpy=0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.train = common.function(agent.train)\n",
    "agent.train_step_counter.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = env_py.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a driver to collect experience.\n",
    "collect_driver = py_driver.PyDriver(\n",
    "    env_py,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "        agent.collect_policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 210: loss = 19.53536605834961\n",
      "step = 210: Average Return = -23.700000762939453\n",
      "step = 220: loss = 11.424781799316406\n",
      "step = 220: Average Return = -15.350000381469727\n",
      "step = 230: loss = 10.36780071258545\n",
      "step = 230: Average Return = -8.84000015258789\n",
      "step = 240: loss = 35.720584869384766\n",
      "step = 240: Average Return = -22.579999923706055\n",
      "step = 250: loss = 24.64096450805664\n",
      "step = 250: Average Return = -16.68000030517578\n",
      "step = 260: loss = 26.593515396118164\n",
      "step = 260: Average Return = -14.800000190734863\n",
      "step = 270: loss = 13.739842414855957\n",
      "step = 270: Average Return = -17.290000915527344\n",
      "step = 280: loss = 20.38434410095215\n",
      "step = 280: Average Return = -22.25\n",
      "step = 290: loss = 31.558549880981445\n",
      "step = 290: Average Return = -24.309999465942383\n",
      "step = 300: loss = 18.185771942138672\n",
      "step = 300: Average Return = -23.190000534057617\n",
      "step = 310: loss = 15.754101753234863\n",
      "step = 310: Average Return = -22.979999542236328\n",
      "step = 320: loss = 41.721763610839844\n",
      "step = 320: Average Return = -6.809999942779541\n",
      "step = 330: loss = 18.91501235961914\n",
      "step = 330: Average Return = -8.079999923706055\n",
      "step = 340: loss = 19.314048767089844\n",
      "step = 340: Average Return = -15.149999618530273\n",
      "step = 350: loss = 5.1319990158081055\n",
      "step = 350: Average Return = -16.06999969482422\n",
      "step = 360: loss = 27.64804458618164\n",
      "step = 360: Average Return = -14.420000076293945\n",
      "step = 370: loss = 26.88051986694336\n",
      "step = 370: Average Return = -17.489999771118164\n",
      "step = 380: loss = 31.531055450439453\n",
      "step = 380: Average Return = -18.670000076293945\n",
      "step = 390: loss = 8.313682556152344\n",
      "step = 390: Average Return = -17.850000381469727\n",
      "step = 400: loss = 20.689990997314453\n",
      "step = 400: Average Return = -14.0\n"
     ]
    }
   ],
   "source": [
    "returns = []\n",
    "for _ in range(200):\n",
    "    \n",
    "  # Collect a few steps and save to the replay buffer.\n",
    "  time_step, _ = collect_driver.run(time_step)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % 10 == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % 10 == 0:\n",
    "    avg_return = compute_avg_return(env, agent.policy, 100)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c997253e9c9db01b2c5cb60d4f99773f770ce139bc1614a18038c034a93da84"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
