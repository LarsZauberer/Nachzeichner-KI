{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 16742592699998966772\n",
       " xla_global_id: -1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im [rlHelloWorld](experiments/rlHelloWorld.ipynb) wurde ein anderes reinforcement learning framework genutzt, welches glaube ich eng mit tensorflow zusammenarbeitet. Die Libraries unten sind aber die originalen von Tensorflow aus dem professionellen Cart Pole Tutorial von Tensorflow [Tutorial](https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.environments import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPS_Game(py_environment.PyEnvironment):\n",
    "    \n",
    "  def __init__(self):\n",
    "    self._action_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
    "    self._observation_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(1,), dtype=np.int32, minimum=[0], maximum=[2], name='observation')\n",
    "    self._state = 0\n",
    "    self._episode_ended = False\n",
    "\n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "\n",
    "  def _reset(self):\n",
    "    self._state = np.random.randint(0, 3)\n",
    "    self._episode_ended = False\n",
    "    return ts.restart(np.array([self._state], dtype=np.int32))\n",
    "\n",
    "  def _step(self, action):\n",
    "\n",
    "    if self._episode_ended:\n",
    "      # The last action ended the episode. Ignore the current action and start\n",
    "      # a new episode.\n",
    "      return self.reset()\n",
    "\n",
    "    # Rock = 0, Paper = 1, Scissors = 2\n",
    "    reward = 0\n",
    "    if action == 0:\n",
    "        if self._state == 1:\n",
    "            reward = -1\n",
    "        elif self._state == 2:\n",
    "            reward = 1\n",
    "    elif action == 1:\n",
    "        if self._state == 0:\n",
    "            reward = 1\n",
    "        elif self._state == 2:\n",
    "            reward = -1\n",
    "    elif action == 2:\n",
    "        if self._state == 0:\n",
    "            reward = -1\n",
    "        elif self._state == 1:\n",
    "            reward = 1\n",
    "    else:\n",
    "        raise ValueError('`action` should be 0 or 1.')\n",
    "    \n",
    "    self._episode_ended = True\n",
    "    return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
    "    \n",
    "\n",
    "    ''' if self._episode_ended or self._state >= 21:\n",
    "      reward = self._state - 21 if self._state <= 21 else -21\n",
    "      return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
    "    else:\n",
    "      return ts.transition(\n",
    "          np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_py = RPS_Game()\n",
    "env = tf_py_environment.TFPyEnvironment(env_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with the environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(2, dtype=int32)),\n",
       " BoundedTensorSpec(shape=(1,), dtype=tf.int32, name='observation', minimum=array([0], dtype=int32), maximum=array([2], dtype=int32)))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_spec(), env.observation_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': BoundedTensorSpec(shape=(1,), dtype=tf.int32, name='observation', minimum=array([0], dtype=int32), maximum=array([2], dtype=int32)),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.time_step_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartPoleEnv = suite_gym.load(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=1),\n",
       " BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cartPoleEnv.action_spec(), cartPoleEnv.observation_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': array(1., dtype=float32),\n",
       " 'observation': array([ 0.00525199,  0.03895897, -0.02802346, -0.04529826], dtype=float32),\n",
       " 'reward': array(0., dtype=float32),\n",
       " 'step_type': array(0, dtype=int32)})"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cartPoleEnv.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
       " 'observation': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
       " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfCartPoleEnv = tf_py_environment.TFPyEnvironment(cartPoleEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
       " 'observation': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0.03948171, 0.01425867, 0.00426048, 0.03866223]], dtype=float32)>,\n",
       " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfCartPoleEnv.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, int)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_py._state, type(env_py._state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (100, 50) # Die Anzahl der Dense Units in einem Layer\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "  return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# its output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent creation\n",
    "Dieser Agent wird nun über das Tensorflow eigene RL System erstellt und nicht über eine dritte Software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    env.time_step_spec(),\n",
    "    env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy # main policy\n",
    "collect_policy = agent.collect_policy # secondary policy to collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(env.time_step_spec(),\n",
    "                                                env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step = env.reset()\n",
    "random_policy.action(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    \n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "            total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return(env, random_policy, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer (Data Contaier) and Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/platform/tfrecord_checkpointer.cc:150]  Initializing TFRecordCheckpointer in /tmp/tmp3v9u49jh.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:386] Loading latest checkpoint from /tmp/tmp3v9u49jh\n",
      "[reverb/cc/platform/default/server.cc:71] Started replay server on port 17554\n"
     ]
    }
   ],
   "source": [
    "table_name = 'uniform_table'\n",
    "replay_buffer_signature = tensor_spec.from_spec(\n",
    "      agent.collect_data_spec)\n",
    "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
    "    replay_buffer_signature)\n",
    "\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=100000,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "    signature=replay_buffer_signature)\n",
    "\n",
    "reverb_server = reverb.Server([table])\n",
    "\n",
    "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    table_name=table_name,\n",
    "    sequence_length=2,\n",
    "    local_server=reverb_server)\n",
    "\n",
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "  replay_buffer.py_client,\n",
    "  table_name,\n",
    "  sequence_length=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TimeStep(\n",
       " {'discount': array(0., dtype=float32),\n",
       "  'observation': array([0], dtype=int32),\n",
       "  'reward': array(0., dtype=float32),\n",
       "  'step_type': array(2, dtype=int32)}),\n",
       " ())"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_driver.PyDriver(\n",
    "    env_py,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "        random_policy, use_tf_function=True),\n",
    "        [rb_observer],\n",
    "        max_steps=100\n",
    "    ).run(env_py.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Trajectory(\n",
       " {'action': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1], dtype=int32)>,\n",
       "  'discount': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 1.], dtype=float32)>,\n",
       "  'next_step_type': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 0], dtype=int32)>,\n",
       "  'observation': <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n",
       " array([[2],\n",
       "        [2]], dtype=int32)>,\n",
       "  'policy_info': (),\n",
       "  'reward': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 0.], dtype=float32)>,\n",
       "  'step_type': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 2], dtype=int32)>}),\n",
       " SampleInfo(key=<tf.Tensor: shape=(2,), dtype=uint64, numpy=array([68393806668783593, 68393806668783593], dtype=uint64)>, probability=<tf.Tensor: shape=(2,), dtype=float64, numpy=array([0.01010101, 0.01010101])>, table_size=<tf.Tensor: shape=(2,), dtype=int64, numpy=array([99, 99])>, priority=<tf.Tensor: shape=(2,), dtype=float64, numpy=array([1., 1.])>))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(replay_buffer.as_dataset()).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(Trajectory(\n",
       "{'action': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
       " 'discount': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
       " 'next_step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
       " 'observation': TensorSpec(shape=(64, 2, 1), dtype=tf.int32, name=None),\n",
       " 'policy_info': (),\n",
       " 'reward': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
       " 'step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)}), SampleInfo(key=TensorSpec(shape=(64, 2), dtype=tf.uint64, name=None), probability=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), table_size=TensorSpec(shape=(64, 2), dtype=tf.int64, name=None), priority=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None)))>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f7a280c8100>\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (1143) so Table uniform_table is accessed directly without gRPC.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Trajectory(\n",
       " {'action': <tf.Tensor: shape=(64, 2), dtype=int32, numpy=\n",
       " array([[0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [1, 0],\n",
       "        [0, 0],\n",
       "        [1, 2],\n",
       "        [0, 1],\n",
       "        [1, 1],\n",
       "        [2, 1],\n",
       "        [1, 0],\n",
       "        [0, 0],\n",
       "        [2, 0],\n",
       "        [1, 1],\n",
       "        [1, 0],\n",
       "        [1, 2],\n",
       "        [1, 2],\n",
       "        [0, 2],\n",
       "        [2, 2],\n",
       "        [1, 1],\n",
       "        [2, 2],\n",
       "        [1, 2],\n",
       "        [2, 1],\n",
       "        [2, 2],\n",
       "        [1, 2],\n",
       "        [2, 0],\n",
       "        [1, 1],\n",
       "        [0, 1],\n",
       "        [2, 1],\n",
       "        [1, 0],\n",
       "        [2, 0],\n",
       "        [1, 2],\n",
       "        [0, 2],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [2, 0],\n",
       "        [1, 0],\n",
       "        [0, 0],\n",
       "        [2, 0],\n",
       "        [0, 0],\n",
       "        [2, 2],\n",
       "        [1, 2],\n",
       "        [2, 2],\n",
       "        [2, 0],\n",
       "        [1, 1],\n",
       "        [2, 0],\n",
       "        [1, 2],\n",
       "        [1, 2],\n",
       "        [0, 1],\n",
       "        [1, 1],\n",
       "        [2, 1],\n",
       "        [1, 0],\n",
       "        [2, 1],\n",
       "        [1, 2],\n",
       "        [2, 0],\n",
       "        [2, 1],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [2, 1],\n",
       "        [2, 2],\n",
       "        [2, 2],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [2, 2]], dtype=int32)>,\n",
       "  'discount': <tf.Tensor: shape=(64, 2), dtype=float32, numpy=\n",
       " array([[0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]], dtype=float32)>,\n",
       "  'next_step_type': <tf.Tensor: shape=(64, 2), dtype=int32, numpy=\n",
       " array([[2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0]], dtype=int32)>,\n",
       "  'observation': <tf.Tensor: shape=(64, 2, 1), dtype=int32, numpy=\n",
       " array([[[2],\n",
       "         [2]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[2],\n",
       "         [2]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[0],\n",
       "         [0]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[2],\n",
       "         [2]],\n",
       " \n",
       "        [[0],\n",
       "         [0]],\n",
       " \n",
       "        [[2],\n",
       "         [2]],\n",
       " \n",
       "        [[2],\n",
       "         [2]],\n",
       " \n",
       "        [[2],\n",
       "         [2]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[0],\n",
       "         [0]],\n",
       " \n",
       "        [[2],\n",
       "         [2]],\n",
       " \n",
       "        [[0],\n",
       "         [0]],\n",
       " \n",
       "        [[0],\n",
       "         [0]],\n",
       " \n",
       "        [[2],\n",
       "         [2]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[0],\n",
       "         [0]],\n",
       " \n",
       "        [[0],\n",
       "         [0]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[2],\n",
       "         [2]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[0],\n",
       "         [0]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[2],\n",
       "         [2]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[2],\n",
       "         [2]],\n",
       " \n",
       "        [[0],\n",
       "         [0]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[0],\n",
       "         [0]],\n",
       " \n",
       "        [[2],\n",
       "         [2]],\n",
       " \n",
       "        [[0],\n",
       "         [0]],\n",
       " \n",
       "        [[2],\n",
       "         [2]],\n",
       " \n",
       "        [[2],\n",
       "         [2]],\n",
       " \n",
       "        [[0],\n",
       "         [0]],\n",
       " \n",
       "        [[2],\n",
       "         [2]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[0],\n",
       "         [0]],\n",
       " \n",
       "        [[2],\n",
       "         [2]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[0],\n",
       "         [0]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[2],\n",
       "         [2]],\n",
       " \n",
       "        [[0],\n",
       "         [0]],\n",
       " \n",
       "        [[2],\n",
       "         [2]],\n",
       " \n",
       "        [[2],\n",
       "         [2]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[2],\n",
       "         [2]],\n",
       " \n",
       "        [[0],\n",
       "         [0]],\n",
       " \n",
       "        [[2],\n",
       "         [2]],\n",
       " \n",
       "        [[0],\n",
       "         [0]],\n",
       " \n",
       "        [[1],\n",
       "         [1]],\n",
       " \n",
       "        [[0],\n",
       "         [0]],\n",
       " \n",
       "        [[0],\n",
       "         [0]],\n",
       " \n",
       "        [[1],\n",
       "         [1]]], dtype=int32)>,\n",
       "  'policy_info': (),\n",
       "  'reward': <tf.Tensor: shape=(64, 2), dtype=float32, numpy=\n",
       " array([[ 1.,  0.],\n",
       "        [-1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [-1.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [-1.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [-1.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [-1.,  0.],\n",
       "        [-1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [-1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [-1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [-1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [-1.,  0.],\n",
       "        [-1.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [-1.,  0.],\n",
       "        [-1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [-1.,  0.],\n",
       "        [-1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [-1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [-1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [-1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [ 1.,  0.]], dtype=float32)>,\n",
       "  'step_type': <tf.Tensor: shape=(64, 2), dtype=int32, numpy=\n",
       " array([[0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2],\n",
       "        [0, 2]], dtype=int32)>}),\n",
       " SampleInfo(key=<tf.Tensor: shape=(64, 2), dtype=uint64, numpy=\n",
       " array([[17178525089035421392, 17178525089035421392],\n",
       "        [ 8854350357049923647,  8854350357049923647],\n",
       "        [11868281961579929035, 11868281961579929035],\n",
       "        [13014633478093855772, 13014633478093855772],\n",
       "        [11935810339876088529, 11935810339876088529],\n",
       "        [16012044262192470839, 16012044262192470839],\n",
       "        [  736726587083857395,   736726587083857395],\n",
       "        [ 6763002789388115571,  6763002789388115571],\n",
       "        [ 3444240926501100379,  3444240926501100379],\n",
       "        [ 5089327702377298915,  5089327702377298915],\n",
       "        [15837138886213748562, 15837138886213748562],\n",
       "        [ 1367461001090314974,  1367461001090314974],\n",
       "        [ 4722914866104096535,  4722914866104096535],\n",
       "        [ 1624833945961666988,  1624833945961666988],\n",
       "        [ 3730034123483933423,  3730034123483933423],\n",
       "        [ 5924506672394433211,  5924506672394433211],\n",
       "        [ 2995731479664933109,  2995731479664933109],\n",
       "        [ 6890772437811119045,  6890772437811119045],\n",
       "        [18011577725453379046, 18011577725453379046],\n",
       "        [ 8504447831051936334,  8504447831051936334],\n",
       "        [18336422928905422304, 18336422928905422304],\n",
       "        [ 2925947899266442286,  2925947899266442286],\n",
       "        [11332637650284251674, 11332637650284251674],\n",
       "        [ 5924506672394433211,  5924506672394433211],\n",
       "        [10178105880309163233, 10178105880309163233],\n",
       "        [ 6763002789388115571,  6763002789388115571],\n",
       "        [17021848157646133682, 17021848157646133682],\n",
       "        [ 1036792466796923314,  1036792466796923314],\n",
       "        [ 3462516819203734197,  3462516819203734197],\n",
       "        [11188839777943835842, 11188839777943835842],\n",
       "        [11435137749886813345, 11435137749886813345],\n",
       "        [17178525089035421392, 17178525089035421392],\n",
       "        [12065914930372460788, 12065914930372460788],\n",
       "        [11910497278116521171, 11910497278116521171],\n",
       "        [ 1377035263621556477,  1377035263621556477],\n",
       "        [ 2771868310929018758,  2771868310929018758],\n",
       "        [ 6613094392363119543,  6613094392363119543],\n",
       "        [ 1367461001090314974,  1367461001090314974],\n",
       "        [13442048173828117371, 13442048173828117371],\n",
       "        [ 5567632094413421833,  5567632094413421833],\n",
       "        [16255157263846788578, 16255157263846788578],\n",
       "        [ 8663704655376534294,  8663704655376534294],\n",
       "        [ 8531964302751190057,  8531964302751190057],\n",
       "        [18011577725453379046, 18011577725453379046],\n",
       "        [11188839777943835842, 11188839777943835842],\n",
       "        [ 2911260786454514369,  2911260786454514369],\n",
       "        [16012044262192470839, 16012044262192470839],\n",
       "        [  736726587083857395,   736726587083857395],\n",
       "        [17417819642332121385, 17417819642332121385],\n",
       "        [ 3444240926501100379,  3444240926501100379],\n",
       "        [12065914930372460788, 12065914930372460788],\n",
       "        [ 3444240926501100379,  3444240926501100379],\n",
       "        [ 5924506672394433211,  5924506672394433211],\n",
       "        [12742949999032379235, 12742949999032379235],\n",
       "        [15619237583545555693, 15619237583545555693],\n",
       "        [ 1088717515883187968,  1088717515883187968],\n",
       "        [13442048173828117371, 13442048173828117371],\n",
       "        [ 8945165890455221658,  8945165890455221658],\n",
       "        [ 3444240926501100379,  3444240926501100379],\n",
       "        [ 1975121570142387026,  1975121570142387026],\n",
       "        [ 8504447831051936334,  8504447831051936334],\n",
       "        [ 1985360625776126127,  1985360625776126127],\n",
       "        [12065914930372460788, 12065914930372460788],\n",
       "        [11332637650284251674, 11332637650284251674]], dtype=uint64)>, probability=<tf.Tensor: shape=(64, 2), dtype=float64, numpy=\n",
       " array([[0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101],\n",
       "        [0.01010101, 0.01010101]])>, table_size=<tf.Tensor: shape=(64, 2), dtype=int64, numpy=\n",
       " array([[99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99],\n",
       "        [99, 99]])>, priority=<tf.Tensor: shape=(64, 2), dtype=float64, numpy=\n",
       " array([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])>))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int32, numpy=0>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the train step.\n",
    "agent.train_step_counter.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2, <tf_agents.policies.greedy_policy.GreedyPolicy at 0x7f7a280a9e20>)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the agent's policy once before training.\n",
    "compute_avg_return(env, agent.policy, 10), agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_return = compute_avg_return(env, agent.policy, 100)\n",
    "returns = [avg_return]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Achtung: Bei den nächsten beiden Zellen muss das <bold>Python Environnement</bold> verwendet werden und nicht das Tensorflow Environnement</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment.\n",
    "time_step = env_py.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a driver to collect experience.\n",
    "collect_driver = py_driver.PyDriver(\n",
    "    env_py,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "      agent.collect_policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 10: loss = 0.4393332600593567\n",
      "step = 10: Average Return = -0.05000000074505806\n",
      "step = 20: loss = 0.48160040378570557\n",
      "step = 20: Average Return = 0.3799999952316284\n",
      "step = 30: loss = 0.5213533639907837\n",
      "step = 30: Average Return = 0.5\n",
      "step = 40: loss = 0.4458102881908417\n",
      "step = 40: Average Return = 0.6600000262260437\n",
      "step = 50: loss = 0.5037575364112854\n",
      "step = 50: Average Return = 0.4000000059604645\n",
      "step = 60: loss = 0.4755828380584717\n",
      "step = 60: Average Return = 0.6899999976158142\n",
      "step = 70: loss = 0.4983588755130768\n",
      "step = 70: Average Return = 0.7200000286102295\n",
      "step = 80: loss = 0.45242875814437866\n",
      "step = 80: Average Return = 0.6200000047683716\n",
      "step = 90: loss = 0.3597545027732849\n",
      "step = 90: Average Return = 0.41999998688697815\n",
      "step = 100: loss = 0.4036809802055359\n",
      "step = 100: Average Return = 0.5699999928474426\n",
      "step = 110: loss = 0.26815348863601685\n",
      "step = 110: Average Return = 1.0\n",
      "step = 120: loss = 0.47258347272872925\n",
      "step = 120: Average Return = 1.0\n",
      "step = 130: loss = 0.30300793051719666\n",
      "step = 130: Average Return = 0.6800000071525574\n",
      "step = 140: loss = 0.3103616535663605\n",
      "step = 140: Average Return = 1.0\n",
      "step = 150: loss = 0.31516575813293457\n",
      "step = 150: Average Return = 1.0\n",
      "step = 160: loss = 0.22615259885787964\n",
      "step = 160: Average Return = 0.6299999952316284\n",
      "step = 170: loss = 0.2292858362197876\n",
      "step = 170: Average Return = 1.0\n",
      "step = 180: loss = 0.26231837272644043\n",
      "step = 180: Average Return = 1.0\n",
      "step = 190: loss = 0.2357615828514099\n",
      "step = 190: Average Return = 1.0\n",
      "step = 200: loss = 0.16479037702083588\n",
      "step = 200: Average Return = 1.0\n"
     ]
    }
   ],
   "source": [
    "for _ in range(200):\n",
    "    \n",
    "  # Collect a few steps and save to the replay buffer.\n",
    "  time_step, _ = collect_driver.run(time_step)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % 10 == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % 10 == 0:\n",
    "    avg_return = compute_avg_return(env, agent.policy, 100)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the AI -> As an interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPS_Game_Interface(RPS_Game):\n",
    "    def _reset(self):\n",
    "        self._state = int(input(\"Enter your choice: \"))\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(np.array([self._state], dtype=np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_py = RPS_Game_Interface()\n",
    "env = tf_py_environment.TFPyEnvironment(env_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation:  tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
      "Selected Action:  tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "Reward Gained: tf.Tensor([1.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "time_step = env.reset()\n",
    "print(\"Observation: \", time_step.observation)\n",
    "action_step = agent.policy.action(time_step)\n",
    "time_step = env.step(action_step.action)\n",
    "print(\"Selected Action: \", action_step.action)\n",
    "print(\"Reward Gained:\", time_step.reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environnement debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
       " 'observation': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n",
       " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = int(input(\"State: \"))\n",
    "action = int(input(\"Action: \"))\n",
    "reward = 0\n",
    "if action == 0:\n",
    "    if state == 1:\n",
    "        reward = -1\n",
    "    elif state == 2:\n",
    "        reward = 1\n",
    "elif action == 1:\n",
    "    if state == 0:\n",
    "        reward = 1\n",
    "    elif state == 2:\n",
    "        reward = -1\n",
    "elif action == 2:\n",
    "    if state == 0:\n",
    "        reward = -1\n",
    "    elif state == 1:\n",
    "        reward = 1\n",
    "else:\n",
    "    raise ValueError('`action` should be 0 or 1.')\n",
    "\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "291d700669bd43c54f0c30a4fcf2f5d664ab06a34b2f6283f605dda78815dbcc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
